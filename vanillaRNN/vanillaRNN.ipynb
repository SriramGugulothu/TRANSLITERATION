{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8273608,"sourceType":"datasetVersion","datasetId":4912633},{"sourceId":8400322,"sourceType":"datasetVersion","datasetId":4998024}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch import nn\nimport pandas as pd\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport copy\nfrom torch.utils.data import Dataset, DataLoader\nimport random","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-16T06:07:14.966302Z","iopub.execute_input":"2024-05-16T06:07:14.966638Z","iopub.status.idle":"2024-05-16T06:07:18.280429Z","shell.execute_reply.started":"2024-05-16T06:07:14.966608Z","shell.execute_reply":"2024-05-16T06:07:18.279357Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"!pip install wandb\nimport wandb\nfrom wandb.keras import WandbCallback\nimport socket\nsocket.setdefaulttimeout(30)\nwandb.login()\nwandb.init(project ='vanillaRNN')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2024-05-16T06:07:18.282017Z","iopub.execute_input":"2024-05-16T06:07:18.282448Z","iopub.status.idle":"2024-05-16T06:07:18.346040Z","shell.execute_reply.started":"2024-05-16T06:07:18.282420Z","shell.execute_reply":"2024-05-16T06:07:18.345034Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"train_csv = \"/kaggle/input/telugu/tel/tel_train.csv\"\ntest_csv = \"/kaggle/input/telugu/tel/tel_test.csv\"\nval_csv = \"/kaggle/input/telugu/tel/tel_valid.csv\"","metadata":{"execution":{"iopub.status.busy":"2024-05-16T06:07:20.367378Z","iopub.execute_input":"2024-05-16T06:07:20.368091Z","iopub.status.idle":"2024-05-16T06:07:20.372135Z","shell.execute_reply.started":"2024-05-16T06:07:20.368060Z","shell.execute_reply":"2024-05-16T06:07:20.371167Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv(train_csv, header=None)\ntrain_input = train_data[0].to_numpy()\ntrain_output = train_data[1].to_numpy()\nval_data = pd.read_csv(val_csv,header = None)\nval_input = val_data[0].to_numpy()\nval_output = val_data[1].to_numpy()\ntest_data = pd.read_csv(test_csv,header= None)\nprint(len(train_input))\nprint(len(train_output))\nprint(len(val_input))\nprint(len(test_data))\nprint(val_input)\nprint(val_output)","metadata":{"execution":{"iopub.status.busy":"2024-05-16T06:07:22.510300Z","iopub.execute_input":"2024-05-16T06:07:22.511398Z","iopub.status.idle":"2024-05-16T06:07:22.672360Z","shell.execute_reply.started":"2024-05-16T06:07:22.511361Z","shell.execute_reply":"2024-05-16T06:07:22.671264Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"51200\n51200\n4096\n4096\n['bheeshmudini' 'vinyasaanni' 'kaavachhunu' ... 'asramam' 'divine' 'dis']\n['భీష్ముడిని' 'విన్యాసాన్ని' 'కావచ్చును' ... 'ఆశ్రమం' 'డివైన్' 'డిస్']\n","output_type":"stream"}]},{"cell_type":"code","source":"print(train_output[0][5]) #the size of input and output is 4096\nmaxi = 0\nt =''\nfor x in val_input:\n    maxi = max(maxi,len(x))\n    if(maxi == len(x)):\n        t=x\n        \nprint(maxi,t)\nt =''\nmaxi =0 \nfor x in val_output:\n    maxi = max(maxi,len(x))\n    if(maxi == len(x)):\n        t=x\n        \nprint(maxi,t)\n    ","metadata":{"execution":{"iopub.status.busy":"2024-05-16T05:02:24.568313Z","iopub.execute_input":"2024-05-16T05:02:24.568858Z","iopub.status.idle":"2024-05-16T05:02:24.581328Z","shell.execute_reply.started":"2024-05-16T05:02:24.568828Z","shell.execute_reply":"2024-05-16T05:02:24.580373Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"ల\n28 paramaanandabharithudayyaadu\n19 పరమానందభరితుడయ్యాడు\n","output_type":"stream"}]},{"cell_type":"code","source":"\ndef pre_processing(train_input,train_output):\n    data = {\n    \"all_characters\" : [],\n    \"char_num_map\" : {},\n    \"num_char_map\" : {},\n    \"source_charToNum\": torch.zeros(len(train_input),30, dtype=torch.int, device=device),\n    \"source_data\" : train_input,\n        \n    \"all_characters_2\" : [],\n    \"char_num_map_2\" : {},\n    \"num_char_map_2\" : {},\n    \"val_charToNum\": torch.zeros(len(train_output),23, dtype=torch.int, device=device),\n    \"target_data\" : train_output,\n    \"source_len\" : 0,\n    \"target_len\" : 0\n }\n    k = 0 \n    l = 0\n    for i in range(0,len(train_input)):\n        train_input[i] = \"{\" + train_input[i] + \"}\"*(29-len(train_input[i]))\n        charToNum = []\n        for char in (train_input[i]):\n            index = 0\n            if(char not in data[\"all_characters\"]):\n                data[\"all_characters\"].append(char)\n                index = data[\"all_characters\"].index(char)\n                data[\"char_num_map\"][char] = index\n                data[\"num_char_map\"][index] = char\n            else:\n                index = data[\"all_characters\"].index(char)\n            \n            charToNum.append(index)\n            \n        my_tensor = torch.tensor(charToNum,device = device)\n        data[\"source_charToNum\"][k] = my_tensor\n        \n        charToNum1 = []\n        \n        train_output[i] = \"{\" + train_output[i] + \"}\"*(22-len(train_output[i]))\n        for char in (train_output[i]):\n            index = 0\n            if(char not in data[\"all_characters_2\"]):\n                data[\"all_characters_2\"].append(char)\n                index = data[\"all_characters_2\"].index(char)\n                data[\"char_num_map_2\"][char] = index\n                data[\"num_char_map_2\"][index] = char\n            else:\n                index = data[\"all_characters_2\"].index(char)\n                \n            charToNum1.append(index)\n            \n        my_tensor1 = torch.tensor(charToNum1,device = device)\n        data[\"val_charToNum\"][k] = my_tensor1\n        \n        k+=1\n    \n    data[\"source_len\"] = len(data[\"all_characters\"])\n    data[\"target_len\"] = len(data[\"all_characters_2\"])\n        \n    return data\n    \n    \ndata = pre_processing(copy.copy(train_input),copy.copy(train_output))\n# print(data[\"all_characters\"])\n# print(data[\"char_num_map\"])\n# print(data[\"num_char_map\"])\n# print(data[\"all_characters_2\"])\n# print(data[\"char_num_map_2\"])\n# print(data[\"num_char_map_2\"])\nprint(data[\"source_charToNum\"])\nprint(data['val_charToNum'])\nprint(data[\"num_char_map_2\"])\nprint(data[\"num_char_map\"])\nprint(train_input[0])\nprint(data['source_len'])\nprint(data['target_len'])\n","metadata":{"execution":{"iopub.status.busy":"2024-05-16T06:07:25.750521Z","iopub.execute_input":"2024-05-16T06:07:25.750871Z","iopub.status.idle":"2024-05-16T06:07:34.301190Z","shell.execute_reply.started":"2024-05-16T06:07:25.750841Z","shell.execute_reply":"2024-05-16T06:07:34.300222Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"tensor([[ 0,  1,  2,  ...,  9,  9,  9],\n        [ 0,  1,  2,  ...,  9,  9,  9],\n        [ 0, 13,  2,  ...,  9,  9,  9],\n        ...,\n        [ 0,  1,  8,  ...,  9,  9,  9],\n        [ 0,  3, 16,  ...,  9,  9,  9],\n        [ 0, 14, 20,  ...,  9,  9,  9]], device='cuda:0', dtype=torch.int32)\ntensor([[ 0,  1,  2,  ..., 10, 10, 10],\n        [ 0,  1, 11,  ..., 10, 10, 10],\n        [ 0, 14,  3,  ..., 10, 10, 10],\n        ...,\n        [ 0,  1, 25,  ..., 10, 10, 10],\n        [ 0,  2, 20,  ..., 10, 10, 10],\n        [ 0, 27, 25,  ..., 10, 10, 10]], device='cuda:0', dtype=torch.int32)\n{0: '{', 1: 'వ', 2: 'ర', 3: '్', 4: 'గ', 5: 'ా', 6: 'ల', 7: 'ి', 8: 'న', 9: 'ే', 10: '}', 11: 'స', 12: 'త', 13: 'ద', 14: 'ఫ', 15: 'య', 16: 'క', 17: 'ట', 18: 'మ', 19: 'ో', 20: 'ూ', 21: 'ళ', 22: 'ప', 23: 'ధ', 24: 'ు', 25: 'ె', 26: 'ం', 27: 'చ', 28: 'ై', 29: 'డ', 30: 'ఖ', 31: 'ఉ', 32: 'ష', 33: 'ఆ', 34: 'ొ', 35: 'శ', 36: 'అ', 37: 'భ', 38: 'ృ', 39: 'ణ', 40: 'హ', 41: 'జ', 42: 'ీ', 43: 'ఇ', 44: 'బ', 45: 'ఐ', 46: 'ఒ', 47: 'ఎ', 48: 'ౌ', 49: 'థ', 50: 'ఈ', 51: 'ఊ', 52: 'ఏ', 53: 'ఢ', 54: 'ఓ', 55: 'ఔ', 56: 'ఞ', 57: 'ఠ', 58: 'ఘ', 59: 'ఛ', 60: 'ః', 61: 'ఝ', 62: 'ఋ', 63: 'ఱ'}\n{0: '{', 1: 'v', 2: 'a', 3: 'r', 4: 'g', 5: 'l', 6: 'i', 7: 'n', 8: 'e', 9: '}', 10: 's', 11: 't', 12: 'd', 13: 'f', 14: 'c', 15: 'm', 16: 'o', 17: 'u', 18: 'w', 19: 'p', 20: 'h', 21: 'k', 22: 'y', 23: 'b', 24: 'j', 25: 'z', 26: 'x', 27: 'q'}\nvargaalavaarine\n28\n64\n","output_type":"stream"}]},{"cell_type":"code","source":"print(train_input[1])\nprint(train_output[1])","metadata":{"execution":{"iopub.status.busy":"2024-05-16T06:07:34.302701Z","iopub.execute_input":"2024-05-16T06:07:34.303009Z","iopub.status.idle":"2024-05-16T06:07:34.307621Z","shell.execute_reply.started":"2024-05-16T06:07:34.302984Z","shell.execute_reply":"2024-05-16T06:07:34.306582Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"vastadira\nవస్తాదిరా\n","output_type":"stream"}]},{"cell_type":"code","source":"\ndef pre_processing_validation(val_input,val_output):\n    data2 = {\n    \"all_characters\" : [],\n    \"char_num_map\" : {},\n    \"num_char_map\" : {},\n    \"source_charToNum\": torch.zeros(len(val_input),30, dtype=torch.int, device=device),\n    \"source_data\" : val_input,\n    \"all_characters_2\" : [],\n    \"char_num_map_2\" : {},\n    \"num_char_map_2\" : {},\n    \"val_charToNum\": torch.zeros(len(val_output),23, dtype=torch.int, device=device),\n    \"target_data\" : val_output,\n    \"source_len\" : 0,\n    \"target_len\" : 0\n }\n    k = 0 \n    l = 0\n    \n    m1 = data[\"char_num_map\"]\n    m2 = data[\"char_num_map_2\"]\n    \n    for i in range(0,len(val_input)):\n        val_input[i] = \"{\" + val_input[i] + \"}\"*(29-len(val_input[i]))\n        charToNum = []\n        for char in (val_input[i]):\n            index = 0\n            if(char not in data2[\"all_characters\"]):\n                data2[\"all_characters\"].append(char)\n                index = m1[char]\n                data2[\"char_num_map\"][char] = index\n                data2[\"num_char_map\"][index] = char\n            else:\n                index = m1[char]\n            \n            charToNum.append(index)\n            \n        my_tensor = torch.tensor(charToNum,device = device)\n        data2[\"source_charToNum\"][k] = my_tensor\n        \n        charToNum1 = []\n        val_output[i] = \"{\" + val_output[i] + \"}\"*(22-len(val_output[i]))\n        for char in (val_output[i]):\n            index = 0\n            if(char not in data2[\"all_characters_2\"]):\n                data2[\"all_characters_2\"].append(char)\n                index = m2[char]\n                data2[\"char_num_map_2\"][char] = index\n                data2[\"num_char_map_2\"][index] = char\n            else:\n                index = m2[char]\n                \n            charToNum1.append(index)\n            \n        my_tensor1 = torch.tensor(charToNum1,device = device)\n        data2[\"val_charToNum\"][k] = my_tensor1\n        \n        k+=1\n    \n    data2[\"source_len\"] = len(data2[\"all_characters\"])\n    data2[\"target_len\"] = len(data2[\"all_characters_2\"])\n        \n    return data2\n    \n    \ndata2 = pre_processing_validation(copy.copy(val_input),copy.copy(val_output))\n# print(data[\"all_characters\"])\n# print(data[\"char_num_map\"])\n# print(data[\"num_char_map\"])\n# print(data[\"all_characters_2\"])\n# print(data[\"char_num_map_2\"])\n# print(data[\"num_char_map_2\"])\nprint(data2[\"num_char_map\"])\nprint(data2[\"source_charToNum\"].shape)\n\nprint(data2[\"num_char_map_2\"])\nprint(data2['val_charToNum'][0])\n\n\nprint(val_input[0])\nprint(data2['source_len'])\nprint(data2['target_len'])\n","metadata":{"execution":{"iopub.status.busy":"2024-05-16T06:07:34.308828Z","iopub.execute_input":"2024-05-16T06:07:34.309214Z","iopub.status.idle":"2024-05-16T06:07:34.867365Z","shell.execute_reply.started":"2024-05-16T06:07:34.309179Z","shell.execute_reply":"2024-05-16T06:07:34.866406Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"{0: '{', 23: 'b', 20: 'h', 8: 'e', 10: 's', 15: 'm', 17: 'u', 12: 'd', 6: 'i', 7: 'n', 9: '}', 1: 'v', 22: 'y', 2: 'a', 21: 'k', 14: 'c', 11: 't', 3: 'r', 19: 'p', 5: 'l', 16: 'o', 4: 'g', 24: 'j', 18: 'w', 26: 'x', 13: 'f', 25: 'z', 27: 'q'}\ntorch.Size([4096, 30])\n{0: '{', 37: 'భ', 42: 'ీ', 32: 'ష', 3: '్', 18: 'మ', 24: 'ు', 29: 'డ', 7: 'ి', 8: 'న', 10: '}', 1: 'వ', 15: 'య', 5: 'ా', 11: 'స', 16: 'క', 27: 'చ', 12: 'త', 2: 'ర', 26: 'ం', 22: 'ప', 6: 'ల', 20: 'ూ', 49: 'థ', 33: 'ఆ', 35: 'శ', 40: 'హ', 19: 'ో', 4: 'గ', 41: 'జ', 13: 'ద', 34: 'ొ', 28: 'ై', 9: 'ే', 46: 'ఒ', 25: 'ె', 17: 'ట', 39: 'ణ', 43: 'ఇ', 38: 'ృ', 54: 'ఓ', 23: 'ధ', 45: 'ఐ', 47: 'ఎ', 36: 'అ', 44: 'బ', 52: 'ఏ', 14: 'ఫ', 31: 'ఉ', 30: 'ఖ', 21: 'ళ', 51: 'ఊ', 48: 'ౌ', 55: 'ఔ', 57: 'ఠ', 58: 'ఘ', 56: 'ఞ', 50: 'ఈ', 59: 'ఛ', 62: 'ఋ', 60: 'ః', 53: 'ఢ'}\ntensor([ 0, 37, 42, 32,  3, 18, 24, 29,  7,  8,  7, 10, 10, 10, 10, 10, 10, 10,\n        10, 10, 10, 10, 10], device='cuda:0', dtype=torch.int32)\nbheeshmudini\n28\n62\n","output_type":"stream"}]},{"cell_type":"code","source":"class MyDataset(Dataset):\n    def __init__(self, x,y):\n        self.source = x\n        self.target = y\n    \n    def __len__(self):\n        return len(self.source)\n    \n    def __getitem__(self, idx):\n        source_data = self.source[idx]\n        target_data = self.target[idx]\n        return source_data, target_data","metadata":{"execution":{"iopub.status.busy":"2024-05-16T06:07:37.433011Z","iopub.execute_input":"2024-05-16T06:07:37.433620Z","iopub.status.idle":"2024-05-16T06:07:37.439712Z","shell.execute_reply.started":"2024-05-16T06:07:37.433587Z","shell.execute_reply":"2024-05-16T06:07:37.438622Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class MyDataset2(Dataset):\n    def __init__(self, x,y):\n        self.source = x\n        self.target = y\n    \n    def __len__(self):\n        return len(self.source)\n    \n    def __getitem__(self, idx):\n        source_data = self.source[idx]\n        target_data = self.target[idx]\n        return source_data, target_data","metadata":{"execution":{"iopub.status.busy":"2024-05-16T06:07:40.503782Z","iopub.execute_input":"2024-05-16T06:07:40.504185Z","iopub.status.idle":"2024-05-16T06:07:40.510757Z","shell.execute_reply.started":"2024-05-16T06:07:40.504152Z","shell.execute_reply":"2024-05-16T06:07:40.509611Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def validationAccuracy(encoder,decoder,batchsize,tf_ratio,cellType,bidirection):\n    \n    dataLoader = dataLoaderFun(\"validation\",batchsize) # dataLoader depending on train or validation\n    \n    encoder.eval()\n    decoder.eval()\n    \n    validation_accuracy = 0\n    validation_loss = 0\n    \n    lossFunction = nn.NLLLoss()\n    \n    for batch_num, (sourceBatch, targetBatch) in enumerate(dataLoader):\n        \n        encoderInitialState = encoder.getInitialState() #hiddenlayers * BatchSize * Neurons\n\n        if(cellType=='LSTM'):\n            encoderInitialState = (encoderInitialState,encoder.getInitialState())\n            \n        if(bidirection == \"Yes\"):\n            reversed_batch = torch.flip(sourceBatch, dims=[1]) # reverse the batch across rows.\n            sourceBatch = (sourceBatch + reversed_batch)//2 \n        \n        Output = []\n        encoder_output, encoderCurrentState = encoder(sourceBatch,encoderInitialState)\n        #print(encoder_output)\n        #success till here\n\n        loss = 0 # decoder starts form here\n\n        outputSeqLen = targetBatch.shape[1] # here you will get as name justified. 40\n\n       \n        #print(targetBatch)\n\n        decoderCurrState = encoderCurrentState\n\n        randNumber = random.random()\n\n        \n\n        for i in range(0,outputSeqLen):\n\n            if(i == 0):\n                decoderInputensor = targetBatch[:, i].reshape(batchsize,1) #32*1\n                #print(dec_input_tensor.shape)\n            else:\n                if randNumber < tf_ratio:\n                    decoderInputensor = targetBatch[:, i].reshape(batchsize, 1) # current batch is passed\n                else:\n                    decoderInputensor = decoderInputensor.reshape(batchsize, 1) # prev result is passed\n\n            #print(curr_target_chars.shape) #32\n            decoderOutput, decoderCurrState = decoder(decoderInputensor,decoderCurrState)\n            #print(decoderOutput.shape) #(32*1*67) but your output is (32*1*65) becz ur output size is 65\n            dummy, topIndeces = decoderOutput.topk(1)  # you will get top vales and their indices.\n            #print(\"topv\", topv)                 \n\n            decoderOutput = decoderOutput[:, -1, :] #it is just reduce the size from (32*1*67) to (32*67)\n            #print(decoderOutput.shape,curr_target_chars.shape)\n            #print(decoderOutput.shape,curr_target_chars.shape)\n\n            curr_target_chars = targetBatch[:, i] #(32)\n            curr_target_chars = curr_target_chars.type(dtype=torch.long)\n            loss+=(lossFunction(decoderOutput, curr_target_chars)) # you are passing 32*67 softmax values to curr_target_chars which has the 32*1\n            \n            decoderInputensor = topIndeces.squeeze().detach()  # here whatever top softmax indeces are present but converted to 1 dimension\n            #print(decoderInputensor.shape)\n            Output.append(decoderInputensor) # softmax values are attached  \n\n        tensor_2d = torch.stack(Output)\n        Output = tensor_2d.t() #it is outside the for loop\n\n        validation_accuracy += (Output == targetBatch).all(dim=1).sum().item() # it is simple just summing up the equal values\n        validation_loss += (loss.item()/outputSeqLen)\n\n        if(batch_num%20 == 0):\n            print(\"bt:\", batch_num, \" loss:\", loss.item()/outputSeqLen)\n        #'k'/24\n        # here you get the actual word letters seqeunces softamx indeces\n        #[[0,1,2],[0,1,2]] = [shr,ram] 32*40\n        #correct = (Output == targetBatch).all(dim=1).sum().item()\n        #accuracy = accuracy + correct\n    \n    encoder.train()\n    decoder.train()\n    print(\"validation_accuracy\",validation_accuracy/40.96)\n    print(\"validation_loss\",validation_loss)\n#     wandb.log({'validation_accuracy':validation_accuracy/40.96})\n#     wandb.log({'validation_loss':validation_loss})","metadata":{"execution":{"iopub.status.busy":"2024-05-16T06:07:42.563061Z","iopub.execute_input":"2024-05-16T06:07:42.563897Z","iopub.status.idle":"2024-05-16T06:07:42.579106Z","shell.execute_reply.started":"2024-05-16T06:07:42.563861Z","shell.execute_reply":"2024-05-16T06:07:42.577778Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class Encoder(nn.Module):\n    \n    def __init__(self,inputDim,embSize,encoderLayers,hiddenLayerNuerons,cellType,batch_size):\n        super(Encoder, self).__init__()\n        self.embedding = nn.Embedding(inputDim, embSize)\n        self.encoderLayers = encoderLayers\n        self.hiddenLayerNuerons = hiddenLayerNuerons\n        self.batch_size = batch_size\n        \n        if(cellType=='GRU'):\n            self.rnn = nn.GRU(embSize,hiddenLayerNuerons,num_layers=encoderLayers, batch_first=True)\n        elif(cellType=='RNN'):\n            self.rnn = nn.RNN(embSize,hiddenLayerNuerons,num_layers=encoderLayers, batch_first=True)\n        else:\n            self.rnn = nn.LSTM(embSize,hiddenLayerNuerons,num_layers=encoderLayers, batch_first=True)\n            \n    def forward(self, currentInput, prevState):\n        embdInput = self.embedding(currentInput)\n        #output, prev_state = self.rnn(embdInput, prevState)\n        return self.rnn(embdInput, prevState)\n    \n    def getInitialState(self):\n        return torch.zeros(self.encoderLayers,self.batch_size,self.hiddenLayerNuerons, device=device)\n    \nclass Decoder(nn.Module):\n    def __init__(self,outputDim,embSize,hiddenLayerNuerons,decoderLayers,cellType,dropout_p):\n        super(Decoder, self).__init__()\n        self.embedding = nn.Embedding(outputDim, embSize)\n        \n        if(cellType == 'GRU'):\n            self.rnn = nn.GRU(embSize,hiddenLayerNuerons,num_layers=decoderLayers, batch_first=True)\n        elif(cellType == 'RNN'):\n            self.rnn = nn.RNN(embSize,hiddenLayerNuerons,num_layers=decoderLayers, batch_first=True)\n        else:\n            self.rnn = nn.LSTM(embSize,hiddenLayerNuerons,num_layers=decoderLayers, batch_first=True)\n            \n        self.fc = nn.Linear(hiddenLayerNuerons, outputDim) # it is useful for mapping the calculation to vocabularu\n        self.softmax = nn.LogSoftmax(dim=2) #output is in 3rd column \n        self.dropout = nn.Dropout(dropout_p)\n\n    def forward(self, currentInput, prevState):\n        embdInput = self.embedding(currentInput)\n        currEmbd = F.relu(embdInput)\n        output, prevState = self.rnn(currEmbd, prevState)\n        output = self.dropout(output)\n        output = self.softmax(self.fc(output)) \n        return output, prevState ","metadata":{"execution":{"iopub.status.busy":"2024-05-16T06:08:13.743913Z","iopub.execute_input":"2024-05-16T06:08:13.744550Z","iopub.status.idle":"2024-05-16T06:08:13.757913Z","shell.execute_reply.started":"2024-05-16T06:08:13.744516Z","shell.execute_reply":"2024-05-16T06:08:13.756626Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# input_dim = data[\"source_len\"]\n# output_dim = data[\"target_len\"]\n# char_embd_dim=64\n# hidden_layer_neurons = 512\n# learning_rate  =0.0001\n# batch_size = 64\n# number_of_layers = 10\n# tf_ratio = 0.2\n# epochs = 50\ntrain(64,5,5,216,'LSTM','Yes',0.4,20,32,1e-4,\"Adam\",0.2)","metadata":{"execution":{"iopub.status.busy":"2024-05-16T06:08:41.357675Z","iopub.execute_input":"2024-05-16T06:08:41.358281Z","iopub.status.idle":"2024-05-16T06:19:02.738706Z","shell.execute_reply.started":"2024-05-16T06:08:41.358244Z","shell.execute_reply":"2024-05-16T06:19:02.737374Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"bt: 0  loss: 4.133873649265455\nbt: 200  loss: 1.7634781547214673\nbt: 400  loss: 1.5949867911960767\nbt: 600  loss: 1.6886531995690388\nbt: 800  loss: 1.7189054074494734\nbt: 1000  loss: 1.6901337996773098\nbt: 1200  loss: 1.6367235598356829\nbt: 1400  loss: 1.4548319940981658\ntrain_accuracy 0.0\ntrain_loss 2709.3578152034634\nbt: 0  loss: 1.1242449387260105\nbt: 20  loss: 1.2463729692542034\nbt: 40  loss: 1.1694212374479875\nbt: 60  loss: 1.339220378709876\nbt: 80  loss: 1.2801880214525305\nbt: 100  loss: 1.2650811568550442\nbt: 120  loss: 1.3159576913584834\nvalidation_accuracy 0.0\nvalidation_loss 159.6927102959674\nbt: 0  loss: 1.5974315145741338\nbt: 200  loss: 1.5424711807914402\nbt: 400  loss: 1.4387835627016814\nbt: 600  loss: 1.6252572432808254\nbt: 800  loss: 1.5022984380307405\nbt: 1000  loss: 1.5458470220151155\nbt: 1200  loss: 1.5715665402619734\nbt: 1400  loss: 1.750567394754161\ntrain_accuracy 0.0\ntrain_loss 2497.6525156601588\nbt: 0  loss: 1.221502221148947\nbt: 20  loss: 1.383120412411897\nbt: 40  loss: 1.1271371426789656\nbt: 60  loss: 1.3677300992219343\nbt: 80  loss: 1.2607623390529468\nbt: 100  loss: 1.260908209759256\nbt: 120  loss: 1.0780259007992952\nvalidation_accuracy 0.0\nvalidation_loss 158.146187160326\nbt: 0  loss: 1.4323393780252207\nbt: 200  loss: 1.4621928671131963\nbt: 400  loss: 1.6473935998004416\nbt: 600  loss: 1.516836581022843\nbt: 800  loss: 1.4086112976074219\nbt: 1000  loss: 1.478791444197945\nbt: 1200  loss: 1.5243526956309443\nbt: 1400  loss: 1.421794725501019\ntrain_accuracy 0.0\ntrain_loss 2423.1435836294468\nbt: 0  loss: 1.1452423593272334\nbt: 20  loss: 1.0545174971870754\nbt: 40  loss: 1.126264240430749\nbt: 60  loss: 1.1578823587168818\nbt: 80  loss: 1.1128889166790505\nbt: 100  loss: 1.204902814782184\nbt: 120  loss: 1.1624964838442595\nvalidation_accuracy 0.0\nvalidation_loss 147.7290432971457\nbt: 0  loss: 1.4173398225203804\nbt: 200  loss: 1.4719913316809612\nbt: 400  loss: 1.5900314994480298\nbt: 600  loss: 1.4898247096849524\nbt: 800  loss: 1.4390792846679688\nbt: 1000  loss: 1.3971567568571672\nbt: 1200  loss: 1.3558756786844004\nbt: 1400  loss: 1.3187010391898777\ntrain_accuracy 0.0\ntrain_loss 2283.8247496563445\nbt: 0  loss: 0.9930913344673489\nbt: 20  loss: 1.0698893173881199\nbt: 40  loss: 1.0777630184007727\nbt: 60  loss: 1.099750104157821\nbt: 80  loss: 1.0484119912852412\nbt: 100  loss: 1.0254485918127971\nbt: 120  loss: 0.9994309466818104\nvalidation_accuracy 0.0\nvalidation_loss 136.53502497465715\nbt: 0  loss: 1.2922112838081692\nbt: 200  loss: 1.4085703310759172\nbt: 400  loss: 1.2564809218696926\nbt: 600  loss: 1.2955049431842307\nbt: 800  loss: 1.3615573385487432\nbt: 1000  loss: 1.329480378524117\nbt: 1200  loss: 1.3419483848240064\nbt: 1400  loss: 1.2605027737824812\ntrain_accuracy 0.0\ntrain_loss 2091.330830200857\nbt: 0  loss: 0.9920140971308169\nbt: 20  loss: 0.9349387624989385\nbt: 40  loss: 0.9009707077689793\nbt: 60  loss: 0.8994013744851818\nbt: 80  loss: 1.01367129450259\nbt: 100  loss: 0.9484878208326257\nbt: 120  loss: 0.9000212627908458\nvalidation_accuracy 0.0732421875\nvalidation_loss 121.92291508550231\nbt: 0  loss: 1.1476759703262993\nbt: 200  loss: 1.3378911225692085\nbt: 400  loss: 1.1875422933827275\nbt: 600  loss: 1.272324105967646\nbt: 800  loss: 1.2958775395932405\nbt: 1000  loss: 1.0277942989183508\nbt: 1200  loss: 1.0375216110892918\nbt: 1400  loss: 1.0112011950948965\ntrain_accuracy 0.01953125\ntrain_loss 1887.1428671297815\nbt: 0  loss: 0.9667878358260446\nbt: 20  loss: 0.8726387023925781\nbt: 40  loss: 0.9206265159275221\nbt: 60  loss: 0.7373549419900646\nbt: 80  loss: 0.9584844838018003\nbt: 100  loss: 0.8090254742166271\nbt: 120  loss: 0.9110002103059188\nvalidation_accuracy 0.1953125\nvalidation_loss 103.80213845294452\nbt: 0  loss: 1.1173341170601223\nbt: 200  loss: 1.0828267802362856\nbt: 400  loss: 1.3807008162788723\nbt: 600  loss: 1.1515857033107593\nbt: 800  loss: 1.1367889901866084\nbt: 1000  loss: 0.869747078937033\nbt: 1200  loss: 0.9493211663287618\nbt: 1400  loss: 1.045826124108356\ntrain_accuracy 0.11328125\ntrain_loss 1694.3214138279786\nbt: 0  loss: 0.9105388807213824\nbt: 20  loss: 0.7626373456872028\nbt: 40  loss: 0.7533872853154722\nbt: 60  loss: 0.7084479124649711\nbt: 80  loss: 0.7764477937117867\nbt: 100  loss: 0.7698181815769362\nbt: 120  loss: 0.9156502433445143\nvalidation_accuracy 1.26953125\nvalidation_loss 94.0169135798579\nbt: 0  loss: 0.8289682968803074\nbt: 200  loss: 1.0742658532184104\nbt: 400  loss: 0.9476822562839674\nbt: 600  loss: 0.9163407864777938\nbt: 800  loss: 1.025321794592816\nbt: 1000  loss: 0.6185755522354789\nbt: 1200  loss: 1.0428135913351309\nbt: 1400  loss: 1.0999972301980723\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[18], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# input_dim = data[\"source_len\"]\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# output_dim = data[\"target_len\"]\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# char_embd_dim=64\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# tf_ratio = 0.2\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# epochs = 50\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m216\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLSTM\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mYes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAdam\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[16], line 86\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(embSize, encoderLayers, decoderLayers, hiddenLayerNuerons, cellType, bidirection, dropout, epochs, batchsize, learningRate, optimizer, tf_ratio)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(batch_num \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m epoch \u001b[38;5;241m==\u001b[39m epochs\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     84\u001b[0m     numToCharConverter(targetBatch,Output,data) \n\u001b[0;32m---> 86\u001b[0m train_accuracy \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mOutput\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtargetBatch\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;66;03m# it is simple just summing up the equal values\u001b[39;00m\n\u001b[1;32m     88\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;241m/\u001b[39msequenceLen)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(batch_num\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"data = pre_processing(copy.copy(train_input),copy.copy(train_output))","metadata":{"execution":{"iopub.status.busy":"2024-05-16T06:08:17.585584Z","iopub.execute_input":"2024-05-16T06:08:17.585919Z","iopub.status.idle":"2024-05-16T06:08:25.456517Z","shell.execute_reply.started":"2024-05-16T06:08:17.585895Z","shell.execute_reply":"2024-05-16T06:08:25.455663Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def dataLoaderFun(dataName,batch_size):\n    if(dataName == 'train'):\n        dataset = MyDataset(data[\"source_charToNum\"],data['val_charToNum'])\n        return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n    else:\n        dataset = MyDataset(data2[\"source_charToNum\"],data2['val_charToNum'])\n        return  DataLoader(dataset, batch_size=batch_size, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-16T06:08:25.458232Z","iopub.execute_input":"2024-05-16T06:08:25.458582Z","iopub.status.idle":"2024-05-16T06:08:25.464371Z","shell.execute_reply.started":"2024-05-16T06:08:25.458552Z","shell.execute_reply":"2024-05-16T06:08:25.463498Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def train(embSize,encoderLayers,decoderLayers,hiddenLayerNuerons,cellType,bidirection,dropout,epochs,batchsize,learningRate,optimizer,tf_ratio):\n    #add optimizer,tf_ratio to wandb parameters\n    \n    dataLoader = dataLoaderFun(\"train\",batchsize) # dataLoader depending on train or validation\n    \n    lossFunction = nn.NLLLoss()\n    \n    encoder = Encoder(data[\"source_len\"],embSize,encoderLayers,hiddenLayerNuerons,cellType,batchsize).to(device)\n    decoder = Decoder(data[\"target_len\"],embSize,hiddenLayerNuerons,encoderLayers,cellType,dropout).to(device)\n    \n    # done till here\n    if(optimizer == 'Adam'):\n        encoderOptimizer = optim.Adam(encoder.parameters(), lr=learningRate)\n        decoderOptimizer = optim.Adam(decoder.parameters(), lr=learningRate)\n    else:\n        encoderOptimizer = optim.NAdam(encoder.parameters(), lr=learningRate)\n        decoderOptimizer = optim.NAdam(decoder.parameters(), lr=learningRate)\n    \n    \n\n    for epoch in range (0,epochs):\n    \n        train_accuracy = 0 \n        train_loss = 0 \n\n        for batch_num, (sourceBatch, targetBatch) in enumerate(dataLoader):\n                        \n            encoderInitialState = encoder.getInitialState() #hiddenlayers * BatchSize * Neurons\n            \n            if(bidirection == \"Yes\"):\n                reversed_batch = torch.flip(sourceBatch, dims=[1]) # reverse the batch across rows.\n                sourceBatch = (sourceBatch + reversed_batch)//2 # adding reversed data to source data by averaging\n            \n            if(cellType == 'LSTM'):\n                encoderInitialState = (encoderInitialState, encoder.getInitialState())\n                \n            encoder_output, encoderCurrentState = encoder(sourceBatch,encoderInitialState)\n            \n            #print(encoder_output)\n            #success till here3\n            \n            \n            loss = 0 # decoder starts form here\n            \n            sequenceLen = targetBatch.shape[1] # here you will get as name justified. 40\n\n            Output = []\n            #print(targetBatch)\n            \n            randNumber = random.random()\n\n            decoderCurrState = encoderCurrentState\n\n            for i in range(0,sequenceLen):\n                \n                if(i == 0):\n                    decoderInput = targetBatch[:, i].reshape(batchsize,1) #32*1\n                    #print(dec_input_tensor.shape)\n                else:\n                    if randNumber < tf_ratio:\n                        decoderInput = targetBatch[:, i].reshape(batchsize, 1) # current batch is passed\n                    else:\n                        decoderInput = decoderInput.reshape(batchsize, 1) # prev result is passed\n\n                #print(targetChars.shape) #32\n                decoderOutput, decoderCurrState = decoder(decoderInput,decoderCurrState)\n                #print(decoderOutput.shape) #(32*1*67) but your output is (32*1*65) becz ur output size is 65\n                dummy, topIndeces = decoderOutput.topk(1)  # you will get top vales and their indices.\n                #print(\"topv\", topv)                    \n                        \n                decoderOutput = decoderOutput[:, -1, :] #it is just reduce the size from (32*1*67) to (32*67)\n                targetChars = targetBatch[:, i] #(32)\n                targetChars = targetChars.type(dtype=torch.long)                \n                loss+=(lossFunction(decoderOutput, targetChars)) # you are passing 32*67 softmax values to targetChars which has the 32*1\n\n                decoderInput = topIndeces.squeeze().detach()  # here whatever top softmax indeces are present but converted to 1 dimension\n                #print(decoderInput.shape)\n                Output.append(decoderInput) # softmax values are attached\n                \n            tensor_2d = torch.stack(Output)\n            Output = tensor_2d.t() #it is outside the for loop\n            #print(Output) #32*40\n            if(batch_num == 0 and epoch == epochs-1):\n                numToCharConverter(targetBatch,Output,data) \n                \n            train_accuracy += (Output == targetBatch).all(dim=1).sum().item() # it is simple just summing up the equal values\n\n            train_loss += (loss.item()/sequenceLen)\n            \n            if(batch_num%200 == 0):\n                print(\"bt:\", batch_num, \" loss:\", loss.item()/sequenceLen)\n            #'k'/24\n            # here you get the actual word letters seqeunces softamx indeces\n            #[[0,1,2],[0,1,2]] = [shr,ram] 32*40\n            #correct = (Output == targetBatch).all(dim=1).sum().item()\n            #accuracy = accuracy + correct\n            encoderOptimizer.zero_grad()\n            decoderOptimizer.zero_grad()\n            loss.backward()\n            encoderOptimizer.step()\n            decoderOptimizer.step()\n            \n        print(\"train_accuracy\",train_accuracy/512)\n        print(\"train_loss\",train_loss)\n#         wandb.log({'train_accuracy':train_accuracy/512})\n#         wandb.log({'train_loss':train_loss})\n        validationAccuracy(encoder,decoder,batchsize,tf_ratio,cellType,bidirection)","metadata":{"execution":{"iopub.status.busy":"2024-05-16T06:08:31.048042Z","iopub.execute_input":"2024-05-16T06:08:31.048381Z","iopub.status.idle":"2024-05-16T06:08:31.067144Z","shell.execute_reply.started":"2024-05-16T06:08:31.048355Z","shell.execute_reply":"2024-05-16T06:08:31.066254Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"\n\n\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n    \n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def numToCharConverter(inputArray,outputArray,data):\n    mp = data['num_char_map_2']\n    t1 = ''\n    t2 = ''\n    for row1, row2 in zip(inputArray,outputArray):\n        t1=''\n        t2=''\n        for e1, e2 in zip(row1,row2):\n            t1+=mp[e1.item()]\n            t2+=mp[e2.item()]\n        print(t1,\" \",t2)\n            \n        \n    ","metadata":{"execution":{"iopub.status.busy":"2024-05-16T06:08:36.248448Z","iopub.execute_input":"2024-05-16T06:08:36.248798Z","iopub.status.idle":"2024-05-16T06:08:36.254916Z","shell.execute_reply.started":"2024-05-16T06:08:36.248769Z","shell.execute_reply":"2024-05-16T06:08:36.253848Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def main_fun():\n    wandb.init(project ='vanillaRNN')\n    params = wandb.config\n    with wandb.init(project = 'vanillaRNN', name='embedding'+str(params.embSize)+'cellType'+params.cellType+'batchSize'+str(params.batchsize)) as run:\n        train(params.embSize,params.encoderLayers,params.decoderLayers,params.hiddenLayerNuerons,params.cellType,params.bidirection,params.dropout,params.epochs,params.batchsize,params.learningRate,params.optimizer,params.tf_ratio)\n    \nsweep_params = {\n    'method' : 'bayes',\n    'name'   : 'DeepLearningAssignment3',\n    'metric' : {\n        'goal' : 'maximize',\n        'name' : 'validation_accuracy',\n    },\n    'parameters' : {\n        'embSize':{'values':[16,32,64]},\n        'encoderLayers':{'values':[1,5,10]},\n        'decoderLayers' : {'values' : [1,5,10]},\n        'hiddenLayerNuerons'   : {'values' : [64,256,512]},\n        'cellType' : {'values' : ['GRU','RNN','LSTM'] } ,\n        'bidirection' : {'values' : ['no','Yes']},\n        'dropout' : {'values' : [0,0.2,0.3]},\n        'epochs'  : {'values': [10,15]},\n        'batchsize' : {'values' : [32,64]},\n        'learningRate' : {'values' : [1e-2,1e-3,1e-4]},\n        'optimizer':{'values' : ['Adam','Nadam']},\n        'tf_ratio' :{'values' : [0.2,0.4,0.5]}\n    }\n}\nsweepId = wandb.sweep(sweep_params,project = 'vanillaRNN')\nwandb.agent(sweepId,function =main_fun,count = 5)\nwandb.finish()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntrain(data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}