{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8273608,"sourceType":"datasetVersion","datasetId":4912633},{"sourceId":8400322,"sourceType":"datasetVersion","datasetId":4998024}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/sriramgugulothu/dl-assignment3?scriptVersionId=177547708\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import torch\nfrom torch import nn\nimport pandas as pd\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport copy\nfrom torch.utils.data import Dataset, DataLoader\nimport random","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-14T03:55:54.709048Z","iopub.execute_input":"2024-05-14T03:55:54.709763Z","iopub.status.idle":"2024-05-14T03:55:54.716512Z","shell.execute_reply.started":"2024-05-14T03:55:54.709715Z","shell.execute_reply":"2024-05-14T03:55:54.715343Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"!pip install wandb\nimport wandb\nfrom wandb.keras import WandbCallback\nimport socket\nsocket.setdefaulttimeout(30)\nwandb.login()\nwandb.init(project ='vanillaRNN')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T03:56:02.591893Z","iopub.execute_input":"2024-05-14T03:56:02.592292Z","iopub.status.idle":"2024-05-14T03:56:02.651037Z","shell.execute_reply.started":"2024-05-14T03:56:02.592261Z","shell.execute_reply":"2024-05-14T03:56:02.649947Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"train_csv = \"/kaggle/input/telugu/tel/tel_train.csv\"\ntest_csv = \"/kaggle/input/telugu/tel/tel_test.csv\"\nval_csv = \"/kaggle/input/telugu/tel/tel_valid.csv\"","metadata":{"execution":{"iopub.status.busy":"2024-05-14T03:56:04.947518Z","iopub.execute_input":"2024-05-14T03:56:04.948187Z","iopub.status.idle":"2024-05-14T03:56:04.952953Z","shell.execute_reply.started":"2024-05-14T03:56:04.948153Z","shell.execute_reply":"2024-05-14T03:56:04.951793Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv(train_csv, header=None)\ntrain_input = train_data[0].to_numpy()\ntrain_output = train_data[1].to_numpy()\nval_data = pd.read_csv(val_csv,header = None)\nval_input = val_data[0].to_numpy()\nval_output = val_data[1].to_numpy()\ntest_data = pd.read_csv(test_csv,header= None)\nprint(len(train_input))\nprint(len(train_output))\nprint(len(val_input))\nprint(len(test_data))\nprint(val_input)\nprint(val_output)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T03:56:08.576761Z","iopub.execute_input":"2024-05-14T03:56:08.577747Z","iopub.status.idle":"2024-05-14T03:56:08.782623Z","shell.execute_reply.started":"2024-05-14T03:56:08.577711Z","shell.execute_reply":"2024-05-14T03:56:08.781675Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"51200\n51200\n4096\n4096\n['bheeshmudini' 'vinyasaanni' 'kaavachhunu' ... 'asramam' 'divine' 'dis']\n['భీష్ముడిని' 'విన్యాసాన్ని' 'కావచ్చును' ... 'ఆశ్రమం' 'డివైన్' 'డిస్']\n","output_type":"stream"}]},{"cell_type":"code","source":"print(train_output[0][5]) #the size of input and output is 4096\nmaxi = 0\nt =''\nfor x in val_input:\n    maxi = max(maxi,len(x))\n    if(maxi == len(x)):\n        t=x\n        \nprint(maxi,t)\nt =''\nmaxi =0 \nfor x in val_output:\n    maxi = max(maxi,len(x))\n    if(maxi == len(x)):\n        t=x\n        \nprint(maxi,t)\n    ","metadata":{"execution":{"iopub.status.busy":"2024-05-14T03:56:12.979136Z","iopub.execute_input":"2024-05-14T03:56:12.979869Z","iopub.status.idle":"2024-05-14T03:56:12.992538Z","shell.execute_reply.started":"2024-05-14T03:56:12.979838Z","shell.execute_reply":"2024-05-14T03:56:12.991616Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"ల\n28 paramaanandabharithudayyaadu\n19 పరమానందభరితుడయ్యాడు\n","output_type":"stream"}]},{"cell_type":"code","source":"\ndef pre_processing(train_input,train_output):\n    data = {\n    \"all_characters\" : [],\n    \"char_num_map\" : {},\n    \"num_char_map\" : {},\n    \"source_charToNum\": torch.zeros(len(train_input),30, dtype=torch.int, device=device),\n    \"source_data\" : train_input,\n        \n    \"all_characters_2\" : [],\n    \"char_num_map_2\" : {},\n    \"num_char_map_2\" : {},\n    \"val_charToNum\": torch.zeros(len(train_output),23, dtype=torch.int, device=device),\n    \"target_data\" : train_output,\n    \"source_len\" : 0,\n    \"target_len\" : 0\n }\n    k = 0 \n    l = 0\n    for i in range(0,len(train_input)):\n        train_input[i] = \"{\" + train_input[i] + \"}\"*(29-len(train_input[i]))\n        charToNum = []\n        for char in (train_input[i]):\n            index = 0\n            if(char not in data[\"all_characters\"]):\n                data[\"all_characters\"].append(char)\n                index = data[\"all_characters\"].index(char)\n                data[\"char_num_map\"][char] = index\n                data[\"num_char_map\"][index] = char\n            else:\n                index = data[\"all_characters\"].index(char)\n            \n            charToNum.append(index)\n            \n        my_tensor = torch.tensor(charToNum,device = device)\n        data[\"source_charToNum\"][k] = my_tensor\n        \n        charToNum1 = []\n        \n        train_output[i] = \"{\" + train_output[i] + \"}\"*(22-len(train_output[i]))\n        for char in (train_output[i]):\n            index = 0\n            if(char not in data[\"all_characters_2\"]):\n                data[\"all_characters_2\"].append(char)\n                index = data[\"all_characters_2\"].index(char)\n                data[\"char_num_map_2\"][char] = index\n                data[\"num_char_map_2\"][index] = char\n            else:\n                index = data[\"all_characters_2\"].index(char)\n                \n            charToNum1.append(index)\n            \n        my_tensor1 = torch.tensor(charToNum1,device = device)\n        data[\"val_charToNum\"][k] = my_tensor1\n        \n        k+=1\n    \n    data[\"source_len\"] = len(data[\"all_characters\"])\n    data[\"target_len\"] = len(data[\"all_characters_2\"])\n        \n    return data\n    \n    \ndata = pre_processing(copy.copy(train_input),copy.copy(train_output))\n# print(data[\"all_characters\"])\n# print(data[\"char_num_map\"])\n# print(data[\"num_char_map\"])\n# print(data[\"all_characters_2\"])\n# print(data[\"char_num_map_2\"])\n# print(data[\"num_char_map_2\"])\nprint(data[\"source_charToNum\"])\nprint(data['val_charToNum'])\nprint(data[\"num_char_map_2\"])\nprint(data[\"num_char_map\"])\nprint(train_input[0])\nprint(data['source_len'])\nprint(data['target_len'])\n","metadata":{"execution":{"iopub.status.busy":"2024-05-14T03:56:23.217371Z","iopub.execute_input":"2024-05-14T03:56:23.218088Z","iopub.status.idle":"2024-05-14T03:56:31.427236Z","shell.execute_reply.started":"2024-05-14T03:56:23.218053Z","shell.execute_reply":"2024-05-14T03:56:31.426273Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"tensor([[ 0,  1,  2,  ...,  9,  9,  9],\n        [ 0,  1,  2,  ...,  9,  9,  9],\n        [ 0, 13,  2,  ...,  9,  9,  9],\n        ...,\n        [ 0,  1,  8,  ...,  9,  9,  9],\n        [ 0,  3, 16,  ...,  9,  9,  9],\n        [ 0, 14, 20,  ...,  9,  9,  9]], device='cuda:0', dtype=torch.int32)\ntensor([[ 0,  1,  2,  ..., 10, 10, 10],\n        [ 0,  1, 11,  ..., 10, 10, 10],\n        [ 0, 14,  3,  ..., 10, 10, 10],\n        ...,\n        [ 0,  1, 25,  ..., 10, 10, 10],\n        [ 0,  2, 20,  ..., 10, 10, 10],\n        [ 0, 27, 25,  ..., 10, 10, 10]], device='cuda:0', dtype=torch.int32)\n{0: '{', 1: 'వ', 2: 'ర', 3: '్', 4: 'గ', 5: 'ా', 6: 'ల', 7: 'ి', 8: 'న', 9: 'ే', 10: '}', 11: 'స', 12: 'త', 13: 'ద', 14: 'ఫ', 15: 'య', 16: 'క', 17: 'ట', 18: 'మ', 19: 'ో', 20: 'ూ', 21: 'ళ', 22: 'ప', 23: 'ధ', 24: 'ు', 25: 'ె', 26: 'ం', 27: 'చ', 28: 'ై', 29: 'డ', 30: 'ఖ', 31: 'ఉ', 32: 'ష', 33: 'ఆ', 34: 'ొ', 35: 'శ', 36: 'అ', 37: 'భ', 38: 'ృ', 39: 'ణ', 40: 'హ', 41: 'జ', 42: 'ీ', 43: 'ఇ', 44: 'బ', 45: 'ఐ', 46: 'ఒ', 47: 'ఎ', 48: 'ౌ', 49: 'థ', 50: 'ఈ', 51: 'ఊ', 52: 'ఏ', 53: 'ఢ', 54: 'ఓ', 55: 'ఔ', 56: 'ఞ', 57: 'ఠ', 58: 'ఘ', 59: 'ఛ', 60: 'ః', 61: 'ఝ', 62: 'ఋ', 63: 'ఱ'}\n{0: '{', 1: 'v', 2: 'a', 3: 'r', 4: 'g', 5: 'l', 6: 'i', 7: 'n', 8: 'e', 9: '}', 10: 's', 11: 't', 12: 'd', 13: 'f', 14: 'c', 15: 'm', 16: 'o', 17: 'u', 18: 'w', 19: 'p', 20: 'h', 21: 'k', 22: 'y', 23: 'b', 24: 'j', 25: 'z', 26: 'x', 27: 'q'}\nvargaalavaarine\n28\n64\n","output_type":"stream"}]},{"cell_type":"code","source":"print(train_input[1])\nprint(train_output[1])","metadata":{"execution":{"iopub.status.busy":"2024-05-14T03:58:53.356522Z","iopub.execute_input":"2024-05-14T03:58:53.356902Z","iopub.status.idle":"2024-05-14T03:58:53.36236Z","shell.execute_reply.started":"2024-05-14T03:58:53.356872Z","shell.execute_reply":"2024-05-14T03:58:53.361276Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"vastadira\nవస్తాదిరా\n","output_type":"stream"}]},{"cell_type":"code","source":"\ndef pre_processing_validation(val_input,val_output):\n    data2 = {\n    \"all_characters\" : [],\n    \"char_num_map\" : {},\n    \"num_char_map\" : {},\n    \"source_charToNum\": torch.zeros(len(val_input),30, dtype=torch.int, device=device),\n    \"source_data\" : val_input,\n    \"all_characters_2\" : [],\n    \"char_num_map_2\" : {},\n    \"num_char_map_2\" : {},\n    \"val_charToNum\": torch.zeros(len(val_output),23, dtype=torch.int, device=device),\n    \"target_data\" : val_output,\n    \"source_len\" : 0,\n    \"target_len\" : 0\n }\n    k = 0 \n    l = 0\n    \n    m1 = data[\"char_num_map\"]\n    m2 = data[\"char_num_map_2\"]\n    \n    for i in range(0,len(val_input)):\n        val_input[i] = \"{\" + val_input[i] + \"}\"*(29-len(val_input[i]))\n        charToNum = []\n        for char in (val_input[i]):\n            index = 0\n            if(char not in data2[\"all_characters\"]):\n                data2[\"all_characters\"].append(char)\n                index = m1[char]\n                data2[\"char_num_map\"][char] = index\n                data2[\"num_char_map\"][index] = char\n            else:\n                index = m1[char]\n            \n            charToNum.append(index)\n            \n        my_tensor = torch.tensor(charToNum,device = device)\n        data2[\"source_charToNum\"][k] = my_tensor\n        \n        charToNum1 = []\n        val_output[i] = \"{\" + val_output[i] + \"}\"*(22-len(val_output[i]))\n        for char in (val_output[i]):\n            index = 0\n            if(char not in data2[\"all_characters_2\"]):\n                data2[\"all_characters_2\"].append(char)\n                index = m2[char]\n                data2[\"char_num_map_2\"][char] = index\n                data2[\"num_char_map_2\"][index] = char\n            else:\n                index = m2[char]\n                \n            charToNum1.append(index)\n            \n        my_tensor1 = torch.tensor(charToNum1,device = device)\n        data2[\"val_charToNum\"][k] = my_tensor1\n        \n        k+=1\n    \n    data2[\"source_len\"] = len(data2[\"all_characters\"])\n    data2[\"target_len\"] = len(data2[\"all_characters_2\"])\n        \n    return data2\n    \n    \ndata2 = pre_processing_validation(copy.copy(val_input),copy.copy(val_output))\n# print(data[\"all_characters\"])\n# print(data[\"char_num_map\"])\n# print(data[\"num_char_map\"])\n# print(data[\"all_characters_2\"])\n# print(data[\"char_num_map_2\"])\n# print(data[\"num_char_map_2\"])\nprint(data2[\"num_char_map\"])\nprint(data2[\"source_charToNum\"].shape)\n\nprint(data2[\"num_char_map_2\"])\nprint(data2['val_charToNum'][0])\n\n\nprint(val_input[0])\nprint(data2['source_len'])\nprint(data2['target_len'])\n","metadata":{"execution":{"iopub.status.busy":"2024-05-14T04:42:14.401363Z","iopub.execute_input":"2024-05-14T04:42:14.401735Z","iopub.status.idle":"2024-05-14T04:42:15.056681Z","shell.execute_reply.started":"2024-05-14T04:42:14.401707Z","shell.execute_reply":"2024-05-14T04:42:15.055709Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"{0: '{', 23: 'b', 20: 'h', 8: 'e', 10: 's', 15: 'm', 17: 'u', 12: 'd', 6: 'i', 7: 'n', 9: '}', 1: 'v', 22: 'y', 2: 'a', 21: 'k', 14: 'c', 11: 't', 3: 'r', 19: 'p', 5: 'l', 16: 'o', 4: 'g', 24: 'j', 18: 'w', 26: 'x', 13: 'f', 25: 'z', 27: 'q'}\ntorch.Size([4096, 30])\n{0: '{', 37: 'భ', 42: 'ీ', 32: 'ష', 3: '్', 18: 'మ', 24: 'ు', 29: 'డ', 7: 'ి', 8: 'న', 10: '}', 1: 'వ', 15: 'య', 5: 'ా', 11: 'స', 16: 'క', 27: 'చ', 12: 'త', 2: 'ర', 26: 'ం', 22: 'ప', 6: 'ల', 20: 'ూ', 49: 'థ', 33: 'ఆ', 35: 'శ', 40: 'హ', 19: 'ో', 4: 'గ', 41: 'జ', 13: 'ద', 34: 'ొ', 28: 'ై', 9: 'ే', 46: 'ఒ', 25: 'ె', 17: 'ట', 39: 'ణ', 43: 'ఇ', 38: 'ృ', 54: 'ఓ', 23: 'ధ', 45: 'ఐ', 47: 'ఎ', 36: 'అ', 44: 'బ', 52: 'ఏ', 14: 'ఫ', 31: 'ఉ', 30: 'ఖ', 21: 'ళ', 51: 'ఊ', 48: 'ౌ', 55: 'ఔ', 57: 'ఠ', 58: 'ఘ', 56: 'ఞ', 50: 'ఈ', 59: 'ఛ', 62: 'ఋ', 60: 'ః', 53: 'ఢ'}\ntensor([ 0, 37, 42, 32,  3, 18, 24, 29,  7,  8,  7, 10, 10, 10, 10, 10, 10, 10,\n        10, 10, 10, 10, 10], device='cuda:0', dtype=torch.int32)\nbheeshmudini\n28\n62\n","output_type":"stream"}]},{"cell_type":"code","source":"class MyDataset(Dataset):\n    def __init__(self, x,y):\n        self.source = x\n        self.target = y\n    \n    def __len__(self):\n        return len(self.source)\n    \n    def __getitem__(self, idx):\n        source_data = self.source[idx]\n        target_data = self.target[idx]\n        return source_data, target_data","metadata":{"execution":{"iopub.status.busy":"2024-05-14T03:59:27.925713Z","iopub.execute_input":"2024-05-14T03:59:27.926482Z","iopub.status.idle":"2024-05-14T03:59:27.932319Z","shell.execute_reply.started":"2024-05-14T03:59:27.926444Z","shell.execute_reply":"2024-05-14T03:59:27.931334Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"class MyDataset2(Dataset):\n    def __init__(self, x,y):\n        self.source = x\n        self.target = y\n    \n    def __len__(self):\n        return len(self.source)\n    \n    def __getitem__(self, idx):\n        source_data = self.source[idx]\n        target_data = self.target[idx]\n        return source_data, target_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def validationAccuracy(encoder,decoder,batchsize):\n    \n    dataLoader = dataLoaderFun(\"validation\",batchsize) # dataLoader depending on train or validation\n    \n    encoder.eval()\n    decoder.eval()\n    \n    validation_accuracy = 0\n    validation_loss = 0\n    \n    lossFunction = nn.NLLLoss()\n    \n    for batch_num, (source_batch, target_batch) in enumerate(dataLoader):\n        \n        encoder_initial_state = encoder.getInitialState() #hiddenlayers * BatchSize * Neurons\n        encoder_output, encoder_current_state = encoder(source_batch,encoder_initial_state)\n        #print(encoder_output)\n        #success till here\n\n        loss = 0 # decoder starts form here\n        correct = 0\n\n        output_seq_len = target_batch.shape[1] # here you will get as name justified. 40\n\n        decoder_actual_output = []\n        #print(target_batch)\n\n        randNumber = random.random()\n\n        decoder_curr_state = encoder_current_state\n\n        for i in range(0,output_seq_len):\n\n            if(i == 0):\n                decoder_input_tensor = target_batch[:, i].reshape(batchsize,1) #32*1\n                #print(dec_input_tensor.shape)\n            else:\n                if randNumber < tf_ratio:\n                    decoder_input_tensor = target_batch[:, i].reshape(batchsize, 1) # current batch is passed\n                else:\n                    decoder_input_tensor = decoder_input_tensor.reshape(batchsize, 1) # prev result is passed\n\n            #print(curr_target_chars.shape) #32\n            decoder_output, decoder_curr_state = decoder(decoder_input_tensor,decoder_curr_state)\n            #print(decoder_output.shape) #(32*1*67) but your output is (32*1*65) becz ur output size is 65\n            topv, topi = decoder_output.topk(1)  # you will get top vales and their indices.\n            #print(\"topv\", topv)\n            decoder_input_tensor = topi.squeeze().detach()  # here whatever top softmax indeces are present but converted to 1 dimension\n            #print(decoder_input_tensor.shape)\n            decoder_actual_output.append(decoder_input_tensor) # softmax values are attached                    \n\n            decoder_output = decoder_output[:, -1, :] #it is just reduce the size from (32*1*67) to (32*67)\n            #print(decoder_output.shape,curr_target_chars.shape)\n            #print(decoder_output.shape,curr_target_chars.shape)\n\n            curr_target_chars = target_batch[:, i] #(32)\n            curr_target_chars = curr_target_chars.type(dtype=torch.long)\n            #print(curr_target_chars)\n\n            loss+=(lossFunction(decoder_output, curr_target_chars)) # you are passing 32*67 softmax values to curr_target_chars which has the 32*1\n\n        tensor_2d = torch.stack(decoder_actual_output)\n        decoder_actual_output = tensor_2d.t() #it is outside the for loop\n\n        validation_accuracy += (decoder_actual_output == target_batch).all(dim=1).sum().item() # it is simple just summing up the equal values\n        validation_loss += (loss.item()/output_seq_len)\n\n        if(batch_num%20 == 0):\n            print(\"bt:\", batch_num, \" loss:\", loss.item()/output_seq_len)\n        #'k'/24\n        # here you get the actual word letters seqeunces softamx indeces\n        #[[0,1,2],[0,1,2]] = [shr,ram] 32*40\n        #correct = (decoder_actual_output == target_batch).all(dim=1).sum().item()\n        #accuracy = accuracy + correct\n    \n    encoder.train()\n    decoder.train()\n    print(\"validation_accuracy\",validation_accuracy/40.96)\n    print(\"validation_loss\",validation_loss)\n    #wandb.log({'validation_accuracy':validation_accuracy/40.96})\n    #wandb.log({'validation_loss':validation_loss})","metadata":{"execution":{"iopub.status.busy":"2024-05-14T04:52:52.731163Z","iopub.execute_input":"2024-05-14T04:52:52.731992Z","iopub.status.idle":"2024-05-14T04:52:52.746018Z","shell.execute_reply.started":"2024-05-14T04:52:52.731953Z","shell.execute_reply":"2024-05-14T04:52:52.745061Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"class Encoder(nn.Module):\n    \n    def __init__(self,inputDim,embSize,encoderLayers,hiddenLayerNuerons,cellType,batch_size):\n        super(Encoder, self).__init__()\n        self.embedding = nn.Embedding(inputDim, embSize)\n        self.encoderLayers = encoderLayers\n        self.hiddenLayerNuerons = hiddenLayerNuerons\n        self.batch_size = batch_size\n        \n        if(cellType=='GRU'):\n            self.rnn = nn.GRU(embSize,hiddenLayerNuerons,num_layers=encoderLayers, batch_first=True)\n            \n    def forward(self, currentInput, prevState):\n        embdInput = self.embedding(currentInput)\n        output, prev_state = self.rnn(embdInput, prevState)\n        return output, prev_state\n    \n    def getInitialState(self):\n        return torch.zeros(self.encoderLayers,self.batch_size,self.hiddenLayerNuerons, device=device)\n    \nclass Decoder(nn.Module):\n    def __init__(self,outputDim,embSize,hiddenLayerNuerons,decoderLayers,cellType):\n        super(Decoder, self).__init__()\n        self.embedding = nn.Embedding(outputDim, embSize)\n        if(cellType==\"GRU\"):\n            self.rnn = nn.GRU(embSize,hiddenLayerNuerons,num_layers=decoderLayers, batch_first=True)\n        self.fc = nn.Linear(hiddenLayerNuerons, outputDim) # it is useful for mapping the calculation to vocabularu\n        self.softmax = nn.LogSoftmax(dim=2) #output is in 3rd column \n\n    def forward(self, current_input, prev_state):\n        embd_input = self.embedding(current_input)\n        curr_embd = F.relu(embd_input)\n        output, prev_state = self.rnn(curr_embd, prev_state)\n        output = self.softmax(self.fc(output)) \n        return output, prev_state ","metadata":{"execution":{"iopub.status.busy":"2024-05-14T03:59:40.257837Z","iopub.execute_input":"2024-05-14T03:59:40.25819Z","iopub.status.idle":"2024-05-14T03:59:40.269263Z","shell.execute_reply.started":"2024-05-14T03:59:40.258161Z","shell.execute_reply":"2024-05-14T03:59:40.268321Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"input_dim = data[\"source_len\"]\noutput_dim = data[\"target_len\"]\nchar_embd_dim=64\nhidden_layer_neurons = 512\nlearning_rate  =0.0001\nbatch_size = 64\nnumber_of_layers = 10\ntf_ratio = 0.2\nepochs = 50\ntrain(64,10,10,512,'GRU','NO',0.4,20,64,1e-4,\"Adam\",0.2)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T04:56:07.706027Z","iopub.execute_input":"2024-05-14T04:56:07.706409Z","iopub.status.idle":"2024-05-14T05:39:23.42863Z","shell.execute_reply.started":"2024-05-14T04:56:07.706378Z","shell.execute_reply":"2024-05-14T05:39:23.427684Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"bt: 0  loss: 4.170014091159986\nbt: 200  loss: 1.5986936817998472\nbt: 400  loss: 1.4894089076829993\nbt: 600  loss: 1.56836634096892\ntrain_accuracy 0.0\ntrain_loss 1276.9349740899133\nbt: 0  loss: 1.2318977687669836\nbt: 20  loss: 1.2830055070960003\nbt: 40  loss: 1.2050791201384172\nbt: 60  loss: 1.179280654243801\nvalidation_accuracy 0.0\nvalidation_loss 77.11809349060056\nbt: 0  loss: 1.4506165877632473\nbt: 200  loss: 1.5490435724673064\nbt: 400  loss: 1.4859537870987602\nbt: 600  loss: 1.516567561937415\ntrain_accuracy 0.0\ntrain_loss 1190.2211844402805\nbt: 0  loss: 1.1580243317977241\nbt: 20  loss: 1.0907040471615999\nbt: 40  loss: 1.1420731751815132\nbt: 60  loss: 1.2795898603356404\nvalidation_accuracy 0.0\nvalidation_loss 75.11708607880965\nbt: 0  loss: 1.4501022670579993\nbt: 200  loss: 1.5170643018639607\nbt: 400  loss: 1.5390278360118037\nbt: 600  loss: 1.571176114289657\ntrain_accuracy 0.0\ntrain_loss 1182.4661463032578\nbt: 0  loss: 1.1511120174242102\nbt: 20  loss: 1.2077122563901155\nbt: 40  loss: 1.1752256310504416\nbt: 60  loss: 1.1778032883353855\nvalidation_accuracy 0.0\nvalidation_loss 74.90693581622581\nbt: 0  loss: 1.5037777112877888\nbt: 200  loss: 1.4634538733440896\nbt: 400  loss: 1.444307741911515\nbt: 600  loss: 1.41204137387483\ntrain_accuracy 0.0\ntrain_loss 1167.036221628605\nbt: 0  loss: 1.072883937669837\nbt: 20  loss: 1.0798499895178753\nbt: 40  loss: 0.9836438220480214\nbt: 60  loss: 1.160107488217561\nvalidation_accuracy 0.0\nvalidation_loss 72.6333751678467\nbt: 0  loss: 1.434659543244735\nbt: 200  loss: 1.4264000602390454\nbt: 400  loss: 1.3941600633704143\nbt: 600  loss: 1.38885265847911\ntrain_accuracy 0.0\ntrain_loss 1120.1269042802899\nbt: 0  loss: 1.0654612002165422\nbt: 20  loss: 1.044725335162619\nbt: 40  loss: 1.0873585576596467\nbt: 60  loss: 1.0688341389531675\nvalidation_accuracy 0.0\nvalidation_loss 68.86289281430452\nbt: 0  loss: 1.3610957601796025\nbt: 200  loss: 1.331264827562415\nbt: 400  loss: 1.2874487172002378\nbt: 600  loss: 1.3302638012429941\ntrain_accuracy 0.001953125\ntrain_loss 1043.2565067125392\nbt: 0  loss: 0.9262293110723081\nbt: 20  loss: 0.9332797009011974\nbt: 40  loss: 1.0058618628460427\nbt: 60  loss: 1.0073775415835173\nvalidation_accuracy 0.0\nvalidation_loss 61.25912807298741\nbt: 0  loss: 1.1881723818571672\nbt: 200  loss: 1.2233509395433508\nbt: 400  loss: 1.111859197201936\nbt: 600  loss: 1.0997700898543648\ntrain_accuracy 0.0078125\ntrain_loss 921.9565743156102\nbt: 0  loss: 0.8625984191894531\nbt: 20  loss: 0.8862415811289912\nbt: 40  loss: 0.938540914784307\nbt: 60  loss: 0.815482181051503\nvalidation_accuracy 0.09765625\nvalidation_loss 53.254472566687525\nbt: 0  loss: 1.043193236641262\nbt: 200  loss: 1.1355255790378735\nbt: 400  loss: 0.962916083957838\nbt: 600  loss: 0.8964914238971212\ntrain_accuracy 0.111328125\ntrain_loss 793.9750143963367\nbt: 0  loss: 0.6401752803636633\nbt: 20  loss: 0.6694080518639606\nbt: 40  loss: 0.7188047326129415\nbt: 60  loss: 0.6687977417655613\nvalidation_accuracy 0.9521484375\nvalidation_loss 44.38432784702468\nbt: 0  loss: 0.9182448179825492\nbt: 200  loss: 0.8856896110202955\nbt: 400  loss: 0.8539863254712976\nbt: 600  loss: 0.8309689397397249\ntrain_accuracy 0.791015625\ntrain_loss 660.7950429087097\nbt: 0  loss: 0.5592841687409774\nbt: 20  loss: 0.5378301454626996\nbt: 40  loss: 0.5203003261400305\nbt: 60  loss: 0.5563729742298955\nvalidation_accuracy 4.00390625\nvalidation_loss 37.05491165492845\nbt: 0  loss: 0.7145170958145804\nbt: 200  loss: 0.7042194034742273\nbt: 400  loss: 0.6502475323884384\nbt: 600  loss: 0.5982539135476818\ntrain_accuracy 3.2421875\ntrain_loss 542.8555205801258\nbt: 0  loss: 0.4289706686268682\nbt: 20  loss: 0.4247451865154764\nbt: 40  loss: 0.4662331290867018\nbt: 60  loss: 0.4601869583129883\nvalidation_accuracy 8.9111328125\nvalidation_loss 31.203413714533266\nbt: 0  loss: 0.6199854145879331\nbt: 200  loss: 0.5539908201798148\nbt: 400  loss: 0.5077816092449686\nbt: 600  loss: 0.4791612210481063\ntrain_accuracy 7.4609375\ntrain_loss 451.61581566022727\nbt: 0  loss: 0.4958201283993928\nbt: 20  loss: 0.4684146383534307\nbt: 40  loss: 0.42916575722072436\nbt: 60  loss: 0.3936200763868249\nvalidation_accuracy 14.0380859375\nvalidation_loss 27.416143355162244\nbt: 0  loss: 0.48361002880594006\nbt: 200  loss: 0.47050078018851904\nbt: 400  loss: 0.45345654694930365\nbt: 600  loss: 0.33473821308301843\ntrain_accuracy 13.02734375\ntrain_loss 385.99397485152525\nbt: 0  loss: 0.3673397561778193\nbt: 20  loss: 0.5220881752345873\nbt: 40  loss: 0.450269657632579\nbt: 60  loss: 0.38583879885466205\nvalidation_accuracy 18.4326171875\nvalidation_loss 24.90761174326358\nbt: 0  loss: 0.5084902721902599\nbt: 200  loss: 0.4139079632966415\nbt: 400  loss: 0.46051688816236414\nbt: 600  loss: 0.3635156465613324\ntrain_accuracy 18.439453125\ntrain_loss 337.2575814412989\nbt: 0  loss: 0.2912466422371242\nbt: 20  loss: 0.3065796727719514\nbt: 40  loss: 0.3562946319580078\nbt: 60  loss: 0.2583304695461107\nvalidation_accuracy 22.802734375\nvalidation_loss 22.737397919530455\nbt: 0  loss: 0.3302705598914105\nbt: 200  loss: 0.2923359248949134\nbt: 400  loss: 0.40103904060695483\nbt: 600  loss: 0.46979526851488196\ntrain_accuracy 23.205078125\ntrain_loss 300.9570927619935\nbt: 0  loss: 0.374968901924465\nbt: 20  loss: 0.2780251295670219\nbt: 40  loss: 0.330163458119268\nbt: 60  loss: 0.2893280153689177\nvalidation_accuracy 28.90625\nvalidation_loss 20.593040507772695\nbt: 0  loss: 0.2837838711945907\nbt: 200  loss: 0.3163300804469896\nbt: 400  loss: 0.2715941097425378\nbt: 600  loss: 0.3190377484197202\ntrain_accuracy 27.78515625\ntrain_loss 267.6266802912172\nbt: 0  loss: 0.29899246796317724\nbt: 20  loss: 0.3924952382626741\nbt: 40  loss: 0.3258991656096085\nbt: 60  loss: 0.2850100683129352\nvalidation_accuracy 28.5888671875\nvalidation_loss 20.550178258315373\nbt: 0  loss: 0.38951956707498303\nbt: 200  loss: 0.3457589356795601\nbt: 400  loss: 0.24600657172825025\nbt: 600  loss: 0.3388197318367336\ntrain_accuracy 30.978515625\ntrain_loss 247.21686688713393\nbt: 0  loss: 0.3338936308155889\nbt: 20  loss: 0.3063061755636464\nbt: 40  loss: 0.26501740580019745\nbt: 60  loss: 0.2603233586186948\nvalidation_accuracy 32.51953125\nvalidation_loss 19.090013047923218\nbt: 0  loss: 0.2004640828008237\nbt: 200  loss: 0.29748964309692383\nbt: 400  loss: 0.23724458528601605\nbt: 600  loss: 0.2680657842884893\ntrain_accuracy 34.255859375\ntrain_loss 227.10982728004458\nbt: 0  loss: 0.2765822410583496\nbt: 20  loss: 0.2868147311003312\nbt: 40  loss: 0.2774451089941937\nbt: 60  loss: 0.4097827828448752\nvalidation_accuracy 34.5947265625\nvalidation_loss 18.510260955147118\nbt: 0  loss: 0.18463692457779593\nbt: 200  loss: 0.30029750906902813\nbt: 400  loss: 0.25142265402752423\nbt: 600  loss: 0.21276397290437118\ntrain_accuracy 37.572265625\ntrain_loss 207.2960940132971\nbt: 0  loss: 0.24591483240542206\nbt: 20  loss: 0.26284891626109247\nbt: 40  loss: 0.3474024482395338\nbt: 60  loss: 0.2718620714933976\nvalidation_accuracy 33.203125\nvalidation_loss 18.678351194962215\nbt: 0  loss: 0.31389234376990277\nbt: 200  loss: 0.22311925888061523\nbt: 400  loss: 0.3669361446214759\nbt: 600  loss: 0.3802930583124575\ntrain_accuracy 39.798828125\ntrain_loss 193.62725945141005\nbt: 0  loss: 0.30202627182006836\nbt: 20  loss: 0.29585235015205713\nbt: 40  loss: 0.2867145123689071\nbt: 60  loss: 0.20527829294619354\nvalidation_accuracy 37.3291015625\nvalidation_loss 16.978236074032985\n{పంచచూలి}}}}}}}}}}}}}}}   {పంచచులి}}}}}}}}}}}}}}}\n{రాజ్యమేలుకున్నాయి}}}}}   {రాజ్యమోలుకున్నాయి}}}}}\n{అరికట్టినట్టు}}}}}}}}}   {అరికట్టినట్టు}}}}}}}}}\n{భాదపెడుతున్నాయట}}}}}}}   {భాదపెడుతున్నాయట}}}}}}}\n{దేహకాంతిచే}}}}}}}}}}}}   {దీహకంటిచేచ}}}}}}}}}}}}\n{యిచ్చినాదట}}}}}}}}}}}}   {యిచ్చినాడట}}}}}}}}}}}}\n{సందేశమిచ్చారంటే}}}}}}}   {సందేశమిచచరంంే}}}}}}}}}\n{గీపెట్టింది}}}}}}}}}}}   {గీపెట్టింది}}}}}}}}}}}\n{వజ్రపుపొడి}}}}}}}}}}}}   {వజ్రపుపోడి}}}}}}}}}}}}\n{పరిస్థితునికి}}}}}}}}}   {పరిస్థితోనికి}}}}}}}}}\n{పెట్టడానికే}}}}}}}}}}}   {పెట్టడానికే}}}}}}}}}}}\n{చించుకోకండి}}}}}}}}}}}   {చించుకోకండి}}}}}}}}}}}\n{చేసుకోలేదంటూ}}}}}}}}}}   {చేసుకోలేదంటూ}}}}}}}}}}\n{జన్మనా}}}}}}}}}}}}}}}}   {జన్మన}}}}}}}}}}}}}}}}}\n{లక్ష్యంతోప్రభుత్వం}}}}   {లక్ష్యంతోప్్తు్తవం}}}}\n{మోసగిస్తున్నాయి}}}}}}}   {మోసగిస్తున్నాయి}}}}}}}\n{డిగ్రీలలోకి}}}}}}}}}}}   {డెగ్రీలలోకి}}}}}}}}}}}\n{వ్యయభారం}}}}}}}}}}}}}}   {వ్యయాారం}}}}}}}}}}}}}}\n{మొగ్గుచూపిందట}}}}}}}}}   {మొగ్గుచోపిందట}}}}}}}}}\n{అండర్పైనింగ్స్తో}}}}}}   {ఉండర్పినింగ్స్తో}}}}}}\n{తినిపిస్తాగా}}}}}}}}}}   {తినిపిస్తాగా}}}}}}}}}}\n{కార్యకర్తలానే}}}}}}}}}   {కార్యక్్తాైనే}}}}}}}}}\n{గెలుకుడికి}}}}}}}}}}}}   {గెలుకుడికి}}}}}}}}}}}}\n{శిల్పానిది}}}}}}}}}}}}   {శిల్పానిది}}}}}}}}}}}}\n{మసారా}}}}}}}}}}}}}}}}}   {మసాాా}}}}}}}}}}}}}}}}}\n{మరకు}}}}}}}}}}}}}}}}}}   {మరకు}}}}}}}}}}}}}}}}}}\n{విరవడమే}}}}}}}}}}}}}}}   {విరవదమే}}}}}}}}}}}}}}}\n{తట్టుతుంది}}}}}}}}}}}}   {తట్టుతుంది}}}}}}}}}}}}\n{కార్డర్లకు}}}}}}}}}}}}   {కార్డర్లకు}}}}}}}}}}}}\n{మడమతిప్పడం}}}}}}}}}}}}   {మదమతిప్పడం}}}}}}}}}}}}\n{నెడుతున్నాడని}}}}}}}}}   {నెడుతున్నాడని}}}}}}}}}\n{దీపస్థంభాలుగా}}}}}}}}}   {దీపస్థంభాలుగా}}}}}}}}}\n{ఆత్మఫలం}}}}}}}}}}}}}}}   {ఆత్మాలం}}}}}}}}}}}}}}}\n{చవిచూసారో}}}}}}}}}}}}}   {చవిచూసారో}}}}}}}}}}}}}\n{బండియేనా}}}}}}}}}}}}}}   {బంంియేనా}}}}}}}}}}}}}}\n{అంతలేదమ్మా}}}}}}}}}}}}   {అంటలేదమ్మ}}}}}}}}}}}}}\n{ద్వేషిస్తున్నారు}}}}}}   {ద్వేషిస్తున్నారు}}}}}}\n{రీలితో}}}}}}}}}}}}}}}}   {రీలితో}}}}}}}}}}}}}}}}\n{మాట్లాడుదూ}}}}}}}}}}}}   {మాట్లాడుడూ}}}}}}}}}}}}\n{చెయ్యొద్దు}}}}}}}}}}}}   {చెయ్యోద్ధు}}}}}}}}}}}}\n{నీకెక్కడదని}}}}}}}}}}}   {నీకెక్కడదని}}}}}}}}}}}\n{ఎస్సో}}}}}}}}}}}}}}}}}   {ఎస్సో}}}}}}}}}}}}}}}}}\n{పొవెట్కిన్}}}}}}}}}}}}   {పోవెట్కిన్}}}}}}}}}}}}\n{కేంద్రబింధువైన}}}}}}}}   {కేంద్రపిందువైన}}}}}}}}\n{కక్షసాధింపులకు}}}}}}}}   {కక్షసాధింపులకు}}}}}}}}\n{పక్షపాతయుతంగా}}}}}}}}}   {పక్షపాతముతంగా}}}}}}}}}\n{మునివెళ్ళతో}}}}}}}}}}}   {మునివెళ్లతో}}}}}}}}}}}\n{జననేంద్రియపై}}}}}}}}}}   {జననేంద్రియపై}}}}}}}}}}\n{ప్రకాశకుడయిన}}}}}}}}}}   {ప్రకాశకూడయిన}}}}}}}}}}\n{ఇవ్వబడతాయని}}}}}}}}}}}   {ఇవ్వబడతాయని}}}}}}}}}}}\n{తేజగారికి}}}}}}}}}}}}}   {తేలగారికి}}}}}}}}}}}}}\n{కోరమీసంలో}}}}}}}}}}}}}   {కొరమీశంలో}}}}}}}}}}}}}\n{చినుకులని}}}}}}}}}}}}}   {చినుకులని}}}}}}}}}}}}}\n{కలిసినటించి}}}}}}}}}}}   {కలిసినటించి}}}}}}}}}}}\n{టీసీఏఎస్}}}}}}}}}}}}}}   {ట్సాస్స్}}}}}}}}}}}}}}\n{తెచ్చుకొందో}}}}}}}}}}}   {తెచ్చుకొండో}}}}}}}}}}}\n{సన్నివేశాలే}}}}}}}}}}}   {సన్నివేశాలే}}}}}}}}}}}\n{పట్టుబడితోనే}}}}}}}}}}   {పట్టుబడితోనే}}}}}}}}}}\n{కలెక్టరేట్లోని}}}}}}}}   {కలెకటటరెట్లోని}}}}}}}}\n{నీవలని}}}}}}}}}}}}}}}}   {నీవలని}}}}}}}}}}}}}}}}\n{దానవేంద్ర}}}}}}}}}}}}}   {దననాంద్ర}}}}}}}}}}}}}}\n{బయటపడాలన్నా}}}}}}}}}}}   {బయటపడాలన్న}}}}}}}}}}}}\n{కూప్పకూలిపోయాడు}}}}}}}   {కూప్పకోలిపోయాడు}}}}}}}\n{సుందరాకాండ}}}}}}}}}}}}   {సుందరకండ}}}}}}}}}}}}}}\nbt: 0  loss: 0.1757104085839313\nbt: 200  loss: 0.2030773992123811\nbt: 400  loss: 0.22893246360447095\nbt: 600  loss: 0.20351509425951086\ntrain_accuracy 41.830078125\ntrain_loss 182.18148028332237\nbt: 0  loss: 0.25463367545086407\nbt: 20  loss: 0.18905560866646146\nbt: 40  loss: 0.22541873351387356\nbt: 60  loss: 0.3470362787661345\nvalidation_accuracy 38.7939453125\nvalidation_loss 16.53834624912428\n","output_type":"stream"}]},{"cell_type":"code","source":"data = pre_processing(copy.copy(train_input),copy.copy(train_output))","metadata":{"execution":{"iopub.status.busy":"2024-05-14T03:59:46.004387Z","iopub.execute_input":"2024-05-14T03:59:46.004738Z","iopub.status.idle":"2024-05-14T03:59:53.387752Z","shell.execute_reply.started":"2024-05-14T03:59:46.00471Z","shell.execute_reply":"2024-05-14T03:59:53.386918Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def dataLoaderFun(dataName,batch_size):\n    if(dataName == 'train'):\n        dataset = MyDataset(data[\"source_charToNum\"],data['val_charToNum'])\n        return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n    else:\n        dataset = MyDataset(data2[\"source_charToNum\"],data2['val_charToNum'])\n        return  DataLoader(dataset, batch_size=batch_size, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T04:00:05.975084Z","iopub.execute_input":"2024-05-14T04:00:05.975731Z","iopub.status.idle":"2024-05-14T04:00:05.981598Z","shell.execute_reply.started":"2024-05-14T04:00:05.9757Z","shell.execute_reply":"2024-05-14T04:00:05.980599Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def train(embSize,encoderLayers,decoderLayers,hiddenLayerNuerons,cellType,bidirection,dropout,epochs,batchsize,learningRate,optimizer,tf_ratio):\n    #add optimizer,tf_ratio to wandb parameters\n    \n    dataLoader = dataLoaderFun(\"train\",batchsize) # dataLoader depending on train or validation\n    \n    \n    encoder = Encoder(data[\"source_len\"],embSize,encoderLayers,hiddenLayerNuerons,cellType,batchsize).to(device)\n    decoder = Decoder(data[\"target_len\"],embSize,hiddenLayerNuerons,decoderLayers,cellType).to(device)\n    \n    # done till here\n    if(optimizer == 'Adam'):\n        encoderOptimizer = optim.Adam(encoder.parameters(), lr=learningRate)\n        decoderOptimizer = optim.Adam(decoder.parameters(), lr=learningRate)\n    else:\n        encoderOptimizer = optim.NAdam(encoder.parameters(), lr=learningRate)\n        decoderOptimizer = optim.NAdam(decoder.parameters(), lr=learningRate)\n    \n    lossFunction = nn.NLLLoss()\n\n    for epoch in range (0,epochs):\n    \n        train_accuracy = 0 \n        train_loss = 0 \n\n        for batch_num, (source_batch, target_batch) in enumerate(dataLoader):\n                        \n            encoder_initial_state = encoder.getInitialState() #hiddenlayers * BatchSize * Neurons\n            \n            encoder_output, encoder_current_state = encoder(source_batch,encoder_initial_state)\n            #print(encoder_output)\n            #success till here\n            \n            loss = 0 # decoder starts form here\n            correct = 0\n            \n            output_seq_len = target_batch.shape[1] # here you will get as name justified. 40\n\n            decoder_actual_output = []\n            #print(target_batch)\n            \n            randNumber = random.random()\n\n            decoder_curr_state = encoder_current_state\n\n            for i in range(0,output_seq_len):\n                \n                if(i == 0):\n                    decoder_input_tensor = target_batch[:, i].reshape(batchsize,1) #32*1\n                    #print(dec_input_tensor.shape)\n                else:\n                    if randNumber < tf_ratio:\n                        decoder_input_tensor = target_batch[:, i].reshape(batchsize, 1) # current batch is passed\n                    else:\n                        decoder_input_tensor = decoder_input_tensor.reshape(batchsize, 1) # prev result is passed\n\n                #print(curr_target_chars.shape) #32\n                decoder_output, decoder_curr_state = decoder(decoder_input_tensor,decoder_curr_state)\n                #print(decoder_output.shape) #(32*1*67) but your output is (32*1*65) becz ur output size is 65\n                topv, topi = decoder_output.topk(1)  # you will get top vales and their indices.\n                #print(\"topv\", topv)\n                decoder_input_tensor = topi.squeeze().detach()  # here whatever top softmax indeces are present but converted to 1 dimension\n                #print(decoder_input_tensor.shape)\n                decoder_actual_output.append(decoder_input_tensor) # softmax values are attached                    \n                        \n                decoder_output = decoder_output[:, -1, :] #it is just reduce the size from (32*1*67) to (32*67)\n                #print(decoder_output.shape,curr_target_chars.shape)\n                #print(decoder_output.shape,curr_target_chars.shape)\n\n                curr_target_chars = target_batch[:, i] #(32)\n                curr_target_chars = curr_target_chars.type(dtype=torch.long)\n                #print(curr_target_chars)\n                \n                loss+=(lossFunction(decoder_output, curr_target_chars)) # you are passing 32*67 softmax values to curr_target_chars which has the 32*1\n                \n            tensor_2d = torch.stack(decoder_actual_output)\n            decoder_actual_output = tensor_2d.t() #it is outside the for loop\n            #print(decoder_actual_output) #32*40\n            if(batch_num == 0 and epoch == epochs-1):\n                numToCharConverter(target_batch,decoder_actual_output,data) \n                \n            train_accuracy += (decoder_actual_output == target_batch).all(dim=1).sum().item() # it is simple just summing up the equal values\n            train_loss += (loss.item()/output_seq_len)\n            \n            if(batch_num%200 == 0):\n                print(\"bt:\", batch_num, \" loss:\", loss.item()/output_seq_len)\n            #'k'/24\n            # here you get the actual word letters seqeunces softamx indeces\n            #[[0,1,2],[0,1,2]] = [shr,ram] 32*40\n            #correct = (decoder_actual_output == target_batch).all(dim=1).sum().item()\n            #accuracy = accuracy + correct\n            encoderOptimizer.zero_grad()\n            decoderOptimizer.zero_grad()\n            loss.backward()\n            encoderOptimizer.step()\n            decoderOptimizer.step()\n            \n        print(\"train_accuracy\",train_accuracy/512)\n        print(\"train_loss\",train_loss)\n        #wandb.log({'train_accuracy':train_accuracy/512})\n        #wandb.log({'train_loss':train_loss})\n        validationAccuracy(encoder,decoder,batchsize)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T04:55:47.773804Z","iopub.execute_input":"2024-05-14T04:55:47.774176Z","iopub.status.idle":"2024-05-14T04:55:47.79247Z","shell.execute_reply.started":"2024-05-14T04:55:47.774145Z","shell.execute_reply":"2024-05-14T04:55:47.791619Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"\n\n\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n    \n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def numToCharConverter(inputArray,outputArray,data):\n    mp = data['num_char_map_2']\n    t1 = ''\n    t2 = ''\n    for row1, row2 in zip(inputArray,outputArray):\n        t1=''\n        t2=''\n        for e1, e2 in zip(row1,row2):\n            t1+=mp[e1.item()]\n            t2+=mp[e2.item()]\n        print(t1,\" \",t2)\n            \n        \n    ","metadata":{"execution":{"iopub.status.busy":"2024-05-14T04:00:16.014681Z","iopub.execute_input":"2024-05-14T04:00:16.01505Z","iopub.status.idle":"2024-05-14T04:00:16.021287Z","shell.execute_reply.started":"2024-05-14T04:00:16.015022Z","shell.execute_reply":"2024-05-14T04:00:16.020343Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def main_fun():\n    wandb.init(project ='vanillaRNN')\n    params = wandb.config\n    with wandb.init(project = 'vanillaRNN', name='embedding'+str(params.embSize)+'cellType'+params.cellType+'batchSize'+str(params.batchsize)) as run:\n        train(params.embSize,params.encoderLayers,params.decoderLayers,params.hiddenLayerNuerons,params.cellType,params.bidirection,params.dropout,params.epochs,params.batchsize,params.learningRate)\n    \nsweep_params = {\n    'method' : 'bayes',\n    'name'   : 'DeepLearningAssignment3',\n    'metric' : {\n        'goal' : 'maximize',\n        'name' : 'validation_accuracy',\n    },\n    'parameters' : {\n        'embSize':{'values':[64]},\n        'encoderLayers':{'values':[5,10]},\n        'decoderLayers' : {'values' : [5,10]},\n        'hiddenLayerNuerons'   : {'values' : [216,512]},\n        'cellType' : {'values' : ['RNN','GRU','LSTM'] } ,\n        'bidirection' : {'values' : ['yes','no']},\n        'dropout' : {'values' : [0.2,0.3]},\n        'epochs'  : {'values': [10,30,50,100]},\n        'batchsize' : {'values' : [32,64]},\n        'learningRate' : {'values' : [1e-2,1e-3]}\n    }\n}\nsweepId = wandb.sweep(sweep_params,project = 'vanillaRNN')\nwandb.agent(sweepId,function =main_fun,count = 1)\nwandb.finish()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntrain(data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}