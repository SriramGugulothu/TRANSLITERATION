{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8273608,"sourceType":"datasetVersion","datasetId":4912633},{"sourceId":8400322,"sourceType":"datasetVersion","datasetId":4998024}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/sriramgugulothu/dl-assignment3?scriptVersionId=177557141\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import torch\nfrom torch import nn\nimport pandas as pd\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport copy\nfrom torch.utils.data import Dataset, DataLoader\nimport random","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-14T06:41:08.854468Z","iopub.execute_input":"2024-05-14T06:41:08.855157Z","iopub.status.idle":"2024-05-14T06:41:10.965194Z","shell.execute_reply.started":"2024-05-14T06:41:08.855121Z","shell.execute_reply":"2024-05-14T06:41:10.964403Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install wandb\nimport wandb\nfrom wandb.keras import WandbCallback\nimport socket\nsocket.setdefaulttimeout(30)\nwandb.login()\nwandb.init(project ='vanillaRNN')","metadata":{"execution":{"iopub.status.busy":"2024-05-14T06:46:22.647655Z","iopub.execute_input":"2024-05-14T06:46:22.648534Z","iopub.status.idle":"2024-05-14T06:46:56.782739Z","shell.execute_reply.started":"2024-05-14T06:46:22.648487Z","shell.execute_reply":"2024-05-14T06:46:56.781599Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.16.6)\nRequirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.41)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.31.0)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.45.0)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0.1)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (69.0.3)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.4.4)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n","output_type":"stream"},{"name":"stderr","text":"2024-05-14 06:46:35.671274: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-14 06:46:35.671331: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-14 06:46:35.672943: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs23m066\u001b[0m (\u001b[33mdlassignment\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240514_064640-zy4e7xvh</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/dlassignment/vanillaRNN/runs/zy4e7xvh' target=\"_blank\">elated-paper-29</a></strong> to <a href='https://wandb.ai/dlassignment/vanillaRNN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/dlassignment/vanillaRNN' target=\"_blank\">https://wandb.ai/dlassignment/vanillaRNN</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/dlassignment/vanillaRNN/runs/zy4e7xvh' target=\"_blank\">https://wandb.ai/dlassignment/vanillaRNN/runs/zy4e7xvh</a>"},"metadata":{}},{"execution_count":33,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/dlassignment/vanillaRNN/runs/zy4e7xvh?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7a00dd3141f0>"},"metadata":{}}]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T06:41:24.598511Z","iopub.execute_input":"2024-05-14T06:41:24.599299Z","iopub.status.idle":"2024-05-14T06:41:24.651654Z","shell.execute_reply.started":"2024-05-14T06:41:24.599261Z","shell.execute_reply":"2024-05-14T06:41:24.650663Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"train_csv = \"/kaggle/input/telugu/tel/tel_train.csv\"\ntest_csv = \"/kaggle/input/telugu/tel/tel_test.csv\"\nval_csv = \"/kaggle/input/telugu/tel/tel_valid.csv\"","metadata":{"execution":{"iopub.status.busy":"2024-05-14T06:41:27.068851Z","iopub.execute_input":"2024-05-14T06:41:27.069819Z","iopub.status.idle":"2024-05-14T06:41:27.073958Z","shell.execute_reply.started":"2024-05-14T06:41:27.069781Z","shell.execute_reply":"2024-05-14T06:41:27.073106Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv(train_csv, header=None)\ntrain_input = train_data[0].to_numpy()\ntrain_output = train_data[1].to_numpy()\nval_data = pd.read_csv(val_csv,header = None)\nval_input = val_data[0].to_numpy()\nval_output = val_data[1].to_numpy()\ntest_data = pd.read_csv(test_csv,header= None)\nprint(len(train_input))\nprint(len(train_output))\nprint(len(val_input))\nprint(len(test_data))\nprint(val_input)\nprint(val_output)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T06:41:29.265084Z","iopub.execute_input":"2024-05-14T06:41:29.265767Z","iopub.status.idle":"2024-05-14T06:41:29.386182Z","shell.execute_reply.started":"2024-05-14T06:41:29.26573Z","shell.execute_reply":"2024-05-14T06:41:29.385218Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"51200\n51200\n4096\n4096\n['bheeshmudini' 'vinyasaanni' 'kaavachhunu' ... 'asramam' 'divine' 'dis']\n['భీష్ముడిని' 'విన్యాసాన్ని' 'కావచ్చును' ... 'ఆశ్రమం' 'డివైన్' 'డిస్']\n","output_type":"stream"}]},{"cell_type":"code","source":"print(train_output[0][5]) #the size of input and output is 4096\nmaxi = 0\nt =''\nfor x in val_input:\n    maxi = max(maxi,len(x))\n    if(maxi == len(x)):\n        t=x\n        \nprint(maxi,t)\nt =''\nmaxi =0 \nfor x in val_output:\n    maxi = max(maxi,len(x))\n    if(maxi == len(x)):\n        t=x\n        \nprint(maxi,t)\n    ","metadata":{"execution":{"iopub.status.busy":"2024-05-14T06:41:31.422123Z","iopub.execute_input":"2024-05-14T06:41:31.422802Z","iopub.status.idle":"2024-05-14T06:41:31.435177Z","shell.execute_reply.started":"2024-05-14T06:41:31.422767Z","shell.execute_reply":"2024-05-14T06:41:31.434249Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"ల\n28 paramaanandabharithudayyaadu\n19 పరమానందభరితుడయ్యాడు\n","output_type":"stream"}]},{"cell_type":"code","source":"\ndef pre_processing(train_input,train_output):\n    data = {\n    \"all_characters\" : [],\n    \"char_num_map\" : {},\n    \"num_char_map\" : {},\n    \"source_charToNum\": torch.zeros(len(train_input),30, dtype=torch.int, device=device),\n    \"source_data\" : train_input,\n        \n    \"all_characters_2\" : [],\n    \"char_num_map_2\" : {},\n    \"num_char_map_2\" : {},\n    \"val_charToNum\": torch.zeros(len(train_output),23, dtype=torch.int, device=device),\n    \"target_data\" : train_output,\n    \"source_len\" : 0,\n    \"target_len\" : 0\n }\n    k = 0 \n    l = 0\n    for i in range(0,len(train_input)):\n        train_input[i] = \"{\" + train_input[i] + \"}\"*(29-len(train_input[i]))\n        charToNum = []\n        for char in (train_input[i]):\n            index = 0\n            if(char not in data[\"all_characters\"]):\n                data[\"all_characters\"].append(char)\n                index = data[\"all_characters\"].index(char)\n                data[\"char_num_map\"][char] = index\n                data[\"num_char_map\"][index] = char\n            else:\n                index = data[\"all_characters\"].index(char)\n            \n            charToNum.append(index)\n            \n        my_tensor = torch.tensor(charToNum,device = device)\n        data[\"source_charToNum\"][k] = my_tensor\n        \n        charToNum1 = []\n        \n        train_output[i] = \"{\" + train_output[i] + \"}\"*(22-len(train_output[i]))\n        for char in (train_output[i]):\n            index = 0\n            if(char not in data[\"all_characters_2\"]):\n                data[\"all_characters_2\"].append(char)\n                index = data[\"all_characters_2\"].index(char)\n                data[\"char_num_map_2\"][char] = index\n                data[\"num_char_map_2\"][index] = char\n            else:\n                index = data[\"all_characters_2\"].index(char)\n                \n            charToNum1.append(index)\n            \n        my_tensor1 = torch.tensor(charToNum1,device = device)\n        data[\"val_charToNum\"][k] = my_tensor1\n        \n        k+=1\n    \n    data[\"source_len\"] = len(data[\"all_characters\"])\n    data[\"target_len\"] = len(data[\"all_characters_2\"])\n        \n    return data\n    \n    \ndata = pre_processing(copy.copy(train_input),copy.copy(train_output))\n# print(data[\"all_characters\"])\n# print(data[\"char_num_map\"])\n# print(data[\"num_char_map\"])\n# print(data[\"all_characters_2\"])\n# print(data[\"char_num_map_2\"])\n# print(data[\"num_char_map_2\"])\nprint(data[\"source_charToNum\"])\nprint(data['val_charToNum'])\nprint(data[\"num_char_map_2\"])\nprint(data[\"num_char_map\"])\nprint(train_input[0])\nprint(data['source_len'])\nprint(data['target_len'])\n","metadata":{"execution":{"iopub.status.busy":"2024-05-14T06:41:33.029492Z","iopub.execute_input":"2024-05-14T06:41:33.030115Z","iopub.status.idle":"2024-05-14T06:41:40.658862Z","shell.execute_reply.started":"2024-05-14T06:41:33.030082Z","shell.execute_reply":"2024-05-14T06:41:40.657836Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"tensor([[ 0,  1,  2,  ...,  9,  9,  9],\n        [ 0,  1,  2,  ...,  9,  9,  9],\n        [ 0, 13,  2,  ...,  9,  9,  9],\n        ...,\n        [ 0,  1,  8,  ...,  9,  9,  9],\n        [ 0,  3, 16,  ...,  9,  9,  9],\n        [ 0, 14, 20,  ...,  9,  9,  9]], device='cuda:0', dtype=torch.int32)\ntensor([[ 0,  1,  2,  ..., 10, 10, 10],\n        [ 0,  1, 11,  ..., 10, 10, 10],\n        [ 0, 14,  3,  ..., 10, 10, 10],\n        ...,\n        [ 0,  1, 25,  ..., 10, 10, 10],\n        [ 0,  2, 20,  ..., 10, 10, 10],\n        [ 0, 27, 25,  ..., 10, 10, 10]], device='cuda:0', dtype=torch.int32)\n{0: '{', 1: 'వ', 2: 'ర', 3: '్', 4: 'గ', 5: 'ా', 6: 'ల', 7: 'ి', 8: 'న', 9: 'ే', 10: '}', 11: 'స', 12: 'త', 13: 'ద', 14: 'ఫ', 15: 'య', 16: 'క', 17: 'ట', 18: 'మ', 19: 'ో', 20: 'ూ', 21: 'ళ', 22: 'ప', 23: 'ధ', 24: 'ు', 25: 'ె', 26: 'ం', 27: 'చ', 28: 'ై', 29: 'డ', 30: 'ఖ', 31: 'ఉ', 32: 'ష', 33: 'ఆ', 34: 'ొ', 35: 'శ', 36: 'అ', 37: 'భ', 38: 'ృ', 39: 'ణ', 40: 'హ', 41: 'జ', 42: 'ీ', 43: 'ఇ', 44: 'బ', 45: 'ఐ', 46: 'ఒ', 47: 'ఎ', 48: 'ౌ', 49: 'థ', 50: 'ఈ', 51: 'ఊ', 52: 'ఏ', 53: 'ఢ', 54: 'ఓ', 55: 'ఔ', 56: 'ఞ', 57: 'ఠ', 58: 'ఘ', 59: 'ఛ', 60: 'ః', 61: 'ఝ', 62: 'ఋ', 63: 'ఱ'}\n{0: '{', 1: 'v', 2: 'a', 3: 'r', 4: 'g', 5: 'l', 6: 'i', 7: 'n', 8: 'e', 9: '}', 10: 's', 11: 't', 12: 'd', 13: 'f', 14: 'c', 15: 'm', 16: 'o', 17: 'u', 18: 'w', 19: 'p', 20: 'h', 21: 'k', 22: 'y', 23: 'b', 24: 'j', 25: 'z', 26: 'x', 27: 'q'}\nvargaalavaarine\n28\n64\n","output_type":"stream"}]},{"cell_type":"code","source":"print(train_input[1])\nprint(train_output[1])","metadata":{"execution":{"iopub.status.busy":"2024-05-14T06:41:40.667542Z","iopub.execute_input":"2024-05-14T06:41:40.667832Z","iopub.status.idle":"2024-05-14T06:41:40.677307Z","shell.execute_reply.started":"2024-05-14T06:41:40.667801Z","shell.execute_reply":"2024-05-14T06:41:40.676412Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"vastadira\nవస్తాదిరా\n","output_type":"stream"}]},{"cell_type":"code","source":"\ndef pre_processing_validation(val_input,val_output):\n    data2 = {\n    \"all_characters\" : [],\n    \"char_num_map\" : {},\n    \"num_char_map\" : {},\n    \"source_charToNum\": torch.zeros(len(val_input),30, dtype=torch.int, device=device),\n    \"source_data\" : val_input,\n    \"all_characters_2\" : [],\n    \"char_num_map_2\" : {},\n    \"num_char_map_2\" : {},\n    \"val_charToNum\": torch.zeros(len(val_output),23, dtype=torch.int, device=device),\n    \"target_data\" : val_output,\n    \"source_len\" : 0,\n    \"target_len\" : 0\n }\n    k = 0 \n    l = 0\n    \n    m1 = data[\"char_num_map\"]\n    m2 = data[\"char_num_map_2\"]\n    \n    for i in range(0,len(val_input)):\n        val_input[i] = \"{\" + val_input[i] + \"}\"*(29-len(val_input[i]))\n        charToNum = []\n        for char in (val_input[i]):\n            index = 0\n            if(char not in data2[\"all_characters\"]):\n                data2[\"all_characters\"].append(char)\n                index = m1[char]\n                data2[\"char_num_map\"][char] = index\n                data2[\"num_char_map\"][index] = char\n            else:\n                index = m1[char]\n            \n            charToNum.append(index)\n            \n        my_tensor = torch.tensor(charToNum,device = device)\n        data2[\"source_charToNum\"][k] = my_tensor\n        \n        charToNum1 = []\n        val_output[i] = \"{\" + val_output[i] + \"}\"*(22-len(val_output[i]))\n        for char in (val_output[i]):\n            index = 0\n            if(char not in data2[\"all_characters_2\"]):\n                data2[\"all_characters_2\"].append(char)\n                index = m2[char]\n                data2[\"char_num_map_2\"][char] = index\n                data2[\"num_char_map_2\"][index] = char\n            else:\n                index = m2[char]\n                \n            charToNum1.append(index)\n            \n        my_tensor1 = torch.tensor(charToNum1,device = device)\n        data2[\"val_charToNum\"][k] = my_tensor1\n        \n        k+=1\n    \n    data2[\"source_len\"] = len(data2[\"all_characters\"])\n    data2[\"target_len\"] = len(data2[\"all_characters_2\"])\n        \n    return data2\n    \n    \ndata2 = pre_processing_validation(copy.copy(val_input),copy.copy(val_output))\n# print(data[\"all_characters\"])\n# print(data[\"char_num_map\"])\n# print(data[\"num_char_map\"])\n# print(data[\"all_characters_2\"])\n# print(data[\"char_num_map_2\"])\n# print(data[\"num_char_map_2\"])\nprint(data2[\"num_char_map\"])\nprint(data2[\"source_charToNum\"].shape)\n\nprint(data2[\"num_char_map_2\"])\nprint(data2['val_charToNum'][0])\n\n\nprint(val_input[0])\nprint(data2['source_len'])\nprint(data2['target_len'])\n","metadata":{"execution":{"iopub.status.busy":"2024-05-14T06:41:41.939248Z","iopub.execute_input":"2024-05-14T06:41:41.939628Z","iopub.status.idle":"2024-05-14T06:41:42.465507Z","shell.execute_reply.started":"2024-05-14T06:41:41.939598Z","shell.execute_reply":"2024-05-14T06:41:42.464581Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"{0: '{', 23: 'b', 20: 'h', 8: 'e', 10: 's', 15: 'm', 17: 'u', 12: 'd', 6: 'i', 7: 'n', 9: '}', 1: 'v', 22: 'y', 2: 'a', 21: 'k', 14: 'c', 11: 't', 3: 'r', 19: 'p', 5: 'l', 16: 'o', 4: 'g', 24: 'j', 18: 'w', 26: 'x', 13: 'f', 25: 'z', 27: 'q'}\ntorch.Size([4096, 30])\n{0: '{', 37: 'భ', 42: 'ీ', 32: 'ష', 3: '్', 18: 'మ', 24: 'ు', 29: 'డ', 7: 'ి', 8: 'న', 10: '}', 1: 'వ', 15: 'య', 5: 'ా', 11: 'స', 16: 'క', 27: 'చ', 12: 'త', 2: 'ర', 26: 'ం', 22: 'ప', 6: 'ల', 20: 'ూ', 49: 'థ', 33: 'ఆ', 35: 'శ', 40: 'హ', 19: 'ో', 4: 'గ', 41: 'జ', 13: 'ద', 34: 'ొ', 28: 'ై', 9: 'ే', 46: 'ఒ', 25: 'ె', 17: 'ట', 39: 'ణ', 43: 'ఇ', 38: 'ృ', 54: 'ఓ', 23: 'ధ', 45: 'ఐ', 47: 'ఎ', 36: 'అ', 44: 'బ', 52: 'ఏ', 14: 'ఫ', 31: 'ఉ', 30: 'ఖ', 21: 'ళ', 51: 'ఊ', 48: 'ౌ', 55: 'ఔ', 57: 'ఠ', 58: 'ఘ', 56: 'ఞ', 50: 'ఈ', 59: 'ఛ', 62: 'ఋ', 60: 'ః', 53: 'ఢ'}\ntensor([ 0, 37, 42, 32,  3, 18, 24, 29,  7,  8,  7, 10, 10, 10, 10, 10, 10, 10,\n        10, 10, 10, 10, 10], device='cuda:0', dtype=torch.int32)\nbheeshmudini\n28\n62\n","output_type":"stream"}]},{"cell_type":"code","source":"class MyDataset(Dataset):\n    def __init__(self, x,y):\n        self.source = x\n        self.target = y\n    \n    def __len__(self):\n        return len(self.source)\n    \n    def __getitem__(self, idx):\n        source_data = self.source[idx]\n        target_data = self.target[idx]\n        return source_data, target_data","metadata":{"execution":{"iopub.status.busy":"2024-05-14T06:41:45.286254Z","iopub.execute_input":"2024-05-14T06:41:45.287002Z","iopub.status.idle":"2024-05-14T06:41:45.292813Z","shell.execute_reply.started":"2024-05-14T06:41:45.286973Z","shell.execute_reply":"2024-05-14T06:41:45.291894Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class MyDataset2(Dataset):\n    def __init__(self, x,y):\n        self.source = x\n        self.target = y\n    \n    def __len__(self):\n        return len(self.source)\n    \n    def __getitem__(self, idx):\n        source_data = self.source[idx]\n        target_data = self.target[idx]\n        return source_data, target_data","metadata":{"execution":{"iopub.status.busy":"2024-05-14T06:41:47.81209Z","iopub.execute_input":"2024-05-14T06:41:47.812873Z","iopub.status.idle":"2024-05-14T06:41:47.818611Z","shell.execute_reply.started":"2024-05-14T06:41:47.812838Z","shell.execute_reply":"2024-05-14T06:41:47.817462Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def validationAccuracy(encoder,decoder,batchsize,tf_ratio):\n    \n    dataLoader = dataLoaderFun(\"validation\",batchsize) # dataLoader depending on train or validation\n    \n    encoder.eval()\n    decoder.eval()\n    \n    validation_accuracy = 0\n    validation_loss = 0\n    \n    lossFunction = nn.NLLLoss()\n    \n    for batch_num, (source_batch, target_batch) in enumerate(dataLoader):\n        \n        encoder_initial_state = encoder.getInitialState() #hiddenlayers * BatchSize * Neurons\n        encoder_output, encoder_current_state = encoder(source_batch,encoder_initial_state)\n        #print(encoder_output)\n        #success till here\n\n        loss = 0 # decoder starts form here\n        correct = 0\n\n        output_seq_len = target_batch.shape[1] # here you will get as name justified. 40\n\n        decoder_actual_output = []\n        #print(target_batch)\n\n        randNumber = random.random()\n\n        decoder_curr_state = encoder_current_state\n\n        for i in range(0,output_seq_len):\n\n            if(i == 0):\n                decoder_input_tensor = target_batch[:, i].reshape(batchsize,1) #32*1\n                #print(dec_input_tensor.shape)\n            else:\n                if randNumber < tf_ratio:\n                    decoder_input_tensor = target_batch[:, i].reshape(batchsize, 1) # current batch is passed\n                else:\n                    decoder_input_tensor = decoder_input_tensor.reshape(batchsize, 1) # prev result is passed\n\n            #print(curr_target_chars.shape) #32\n            decoder_output, decoder_curr_state = decoder(decoder_input_tensor,decoder_curr_state)\n            #print(decoder_output.shape) #(32*1*67) but your output is (32*1*65) becz ur output size is 65\n            topv, topi = decoder_output.topk(1)  # you will get top vales and their indices.\n            #print(\"topv\", topv)\n            decoder_input_tensor = topi.squeeze().detach()  # here whatever top softmax indeces are present but converted to 1 dimension\n            #print(decoder_input_tensor.shape)\n            decoder_actual_output.append(decoder_input_tensor) # softmax values are attached                    \n\n            decoder_output = decoder_output[:, -1, :] #it is just reduce the size from (32*1*67) to (32*67)\n            #print(decoder_output.shape,curr_target_chars.shape)\n            #print(decoder_output.shape,curr_target_chars.shape)\n\n            curr_target_chars = target_batch[:, i] #(32)\n            curr_target_chars = curr_target_chars.type(dtype=torch.long)\n            #print(curr_target_chars)\n\n            loss+=(lossFunction(decoder_output, curr_target_chars)) # you are passing 32*67 softmax values to curr_target_chars which has the 32*1\n\n        tensor_2d = torch.stack(decoder_actual_output)\n        decoder_actual_output = tensor_2d.t() #it is outside the for loop\n\n        validation_accuracy += (decoder_actual_output == target_batch).all(dim=1).sum().item() # it is simple just summing up the equal values\n        validation_loss += (loss.item()/output_seq_len)\n\n        if(batch_num%20 == 0):\n            print(\"bt:\", batch_num, \" loss:\", loss.item()/output_seq_len)\n        #'k'/24\n        # here you get the actual word letters seqeunces softamx indeces\n        #[[0,1,2],[0,1,2]] = [shr,ram] 32*40\n        #correct = (decoder_actual_output == target_batch).all(dim=1).sum().item()\n        #accuracy = accuracy + correct\n    \n    encoder.train()\n    decoder.train()\n    print(\"validation_accuracy\",validation_accuracy/40.96)\n    print(\"validation_loss\",validation_loss)\n    wandb.log({'validation_accuracy':validation_accuracy/40.96})\n    wandb.log({'validation_loss':validation_loss})","metadata":{"execution":{"iopub.status.busy":"2024-05-14T06:45:51.872217Z","iopub.execute_input":"2024-05-14T06:45:51.872627Z","iopub.status.idle":"2024-05-14T06:45:51.886587Z","shell.execute_reply.started":"2024-05-14T06:45:51.872594Z","shell.execute_reply":"2024-05-14T06:45:51.885689Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"class Encoder(nn.Module):\n    \n    def __init__(self,inputDim,embSize,encoderLayers,hiddenLayerNuerons,cellType,batch_size):\n        super(Encoder, self).__init__()\n        self.embedding = nn.Embedding(inputDim, embSize)\n        self.encoderLayers = encoderLayers\n        self.hiddenLayerNuerons = hiddenLayerNuerons\n        self.batch_size = batch_size\n        \n        if(cellType=='GRU'):\n            self.rnn = nn.GRU(embSize,hiddenLayerNuerons,num_layers=encoderLayers, batch_first=True)\n            \n    def forward(self, currentInput, prevState):\n        embdInput = self.embedding(currentInput)\n        output, prev_state = self.rnn(embdInput, prevState)\n        return output, prev_state\n    \n    def getInitialState(self):\n        return torch.zeros(self.encoderLayers,self.batch_size,self.hiddenLayerNuerons, device=device)\n    \nclass Decoder(nn.Module):\n    def __init__(self,outputDim,embSize,hiddenLayerNuerons,decoderLayers,cellType):\n        super(Decoder, self).__init__()\n        self.embedding = nn.Embedding(outputDim, embSize)\n        if(cellType==\"GRU\"):\n            self.rnn = nn.GRU(embSize,hiddenLayerNuerons,num_layers=decoderLayers, batch_first=True)\n        self.fc = nn.Linear(hiddenLayerNuerons, outputDim) # it is useful for mapping the calculation to vocabularu\n        self.softmax = nn.LogSoftmax(dim=2) #output is in 3rd column \n\n    def forward(self, current_input, prev_state):\n        embd_input = self.embedding(current_input)\n        curr_embd = F.relu(embd_input)\n        output, prev_state = self.rnn(curr_embd, prev_state)\n        output = self.softmax(self.fc(output)) \n        return output, prev_state ","metadata":{"execution":{"iopub.status.busy":"2024-05-14T06:41:58.769967Z","iopub.execute_input":"2024-05-14T06:41:58.770496Z","iopub.status.idle":"2024-05-14T06:41:58.781906Z","shell.execute_reply.started":"2024-05-14T06:41:58.770455Z","shell.execute_reply":"2024-05-14T06:41:58.780844Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"input_dim = data[\"source_len\"]\noutput_dim = data[\"target_len\"]\nchar_embd_dim=64\nhidden_layer_neurons = 512\nlearning_rate  =0.0001\nbatch_size = 64\nnumber_of_layers = 10\ntf_ratio = 0.2\nepochs = 50\n#train(64,1,1,512,'GRU','NO',0.4,20,64,1e-4,\"Adam\",0.2)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T06:45:01.934245Z","iopub.execute_input":"2024-05-14T06:45:01.935261Z","iopub.status.idle":"2024-05-14T06:45:04.254528Z","shell.execute_reply.started":"2024-05-14T06:45:01.935217Z","shell.execute_reply":"2024-05-14T06:45:04.253217Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"bt: 0  loss: 4.114956067956013\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[27], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m tf_ratio \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m\n\u001b[1;32m      9\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[0;32m---> 10\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGRU\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNO\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAdam\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[20], line 57\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(embSize, encoderLayers, decoderLayers, hiddenLayerNuerons, cellType, bidirection, dropout, epochs, batchsize, learningRate, optimizer, tf_ratio)\u001b[0m\n\u001b[1;32m     54\u001b[0m         decoder_input_tensor \u001b[38;5;241m=\u001b[39m decoder_input_tensor\u001b[38;5;241m.\u001b[39mreshape(batchsize, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# prev result is passed\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m#print(curr_target_chars.shape) #32\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m decoder_output, decoder_curr_state \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder_input_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdecoder_curr_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m#print(decoder_output.shape) #(32*1*67) but your output is (32*1*65) becz ur output size is 65\u001b[39;00m\n\u001b[1;32m     59\u001b[0m topv, topi \u001b[38;5;241m=\u001b[39m decoder_output\u001b[38;5;241m.\u001b[39mtopk(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# you will get top vales and their indices.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[14], line 34\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, current_input, prev_state)\u001b[0m\n\u001b[1;32m     32\u001b[0m curr_embd \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(embd_input)\n\u001b[1;32m     33\u001b[0m output, prev_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn(curr_embd, prev_state)\n\u001b[0;32m---> 34\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m) \n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output, prev_state\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"data = pre_processing(copy.copy(train_input),copy.copy(train_output))","metadata":{"execution":{"iopub.status.busy":"2024-05-14T06:42:10.911655Z","iopub.execute_input":"2024-05-14T06:42:10.912554Z","iopub.status.idle":"2024-05-14T06:42:18.342467Z","shell.execute_reply.started":"2024-05-14T06:42:10.91252Z","shell.execute_reply":"2024-05-14T06:42:18.341607Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def dataLoaderFun(dataName,batch_size):\n    if(dataName == 'train'):\n        dataset = MyDataset(data[\"source_charToNum\"],data['val_charToNum'])\n        return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n    else:\n        dataset = MyDataset(data2[\"source_charToNum\"],data2['val_charToNum'])\n        return  DataLoader(dataset, batch_size=batch_size, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T06:42:20.92196Z","iopub.execute_input":"2024-05-14T06:42:20.92238Z","iopub.status.idle":"2024-05-14T06:42:20.928695Z","shell.execute_reply.started":"2024-05-14T06:42:20.922345Z","shell.execute_reply":"2024-05-14T06:42:20.927639Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def train(embSize,encoderLayers,decoderLayers,hiddenLayerNuerons,cellType,bidirection,dropout,epochs,batchsize,learningRate,optimizer,tf_ratio):\n    #add optimizer,tf_ratio to wandb parameters\n    \n    dataLoader = dataLoaderFun(\"train\",batchsize) # dataLoader depending on train or validation\n    \n    \n    encoder = Encoder(data[\"source_len\"],embSize,encoderLayers,hiddenLayerNuerons,cellType,batchsize).to(device)\n    decoder = Decoder(data[\"target_len\"],embSize,hiddenLayerNuerons,encoderLayers,cellType).to(device)\n    \n    # done till here\n    if(optimizer == 'Adam'):\n        encoderOptimizer = optim.Adam(encoder.parameters(), lr=learningRate)\n        decoderOptimizer = optim.Adam(decoder.parameters(), lr=learningRate)\n    else:\n        encoderOptimizer = optim.NAdam(encoder.parameters(), lr=learningRate)\n        decoderOptimizer = optim.NAdam(decoder.parameters(), lr=learningRate)\n    \n    lossFunction = nn.NLLLoss()\n\n    for epoch in range (0,epochs):\n    \n        train_accuracy = 0 \n        train_loss = 0 \n\n        for batch_num, (source_batch, target_batch) in enumerate(dataLoader):\n                        \n            encoder_initial_state = encoder.getInitialState() #hiddenlayers * BatchSize * Neurons\n            \n            encoder_output, encoder_current_state = encoder(source_batch,encoder_initial_state)\n            #print(encoder_output)\n            #success till here\n            \n            loss = 0 # decoder starts form here\n            correct = 0\n            \n            output_seq_len = target_batch.shape[1] # here you will get as name justified. 40\n\n            decoder_actual_output = []\n            #print(target_batch)\n            \n            randNumber = random.random()\n\n            decoder_curr_state = encoder_current_state\n\n            for i in range(0,output_seq_len):\n                \n                if(i == 0):\n                    decoder_input_tensor = target_batch[:, i].reshape(batchsize,1) #32*1\n                    #print(dec_input_tensor.shape)\n                else:\n                    if randNumber < tf_ratio:\n                        decoder_input_tensor = target_batch[:, i].reshape(batchsize, 1) # current batch is passed\n                    else:\n                        decoder_input_tensor = decoder_input_tensor.reshape(batchsize, 1) # prev result is passed\n\n                #print(curr_target_chars.shape) #32\n                decoder_output, decoder_curr_state = decoder(decoder_input_tensor,decoder_curr_state)\n                #print(decoder_output.shape) #(32*1*67) but your output is (32*1*65) becz ur output size is 65\n                topv, topi = decoder_output.topk(1)  # you will get top vales and their indices.\n                #print(\"topv\", topv)\n                decoder_input_tensor = topi.squeeze().detach()  # here whatever top softmax indeces are present but converted to 1 dimension\n                #print(decoder_input_tensor.shape)\n                decoder_actual_output.append(decoder_input_tensor) # softmax values are attached                    \n                        \n                decoder_output = decoder_output[:, -1, :] #it is just reduce the size from (32*1*67) to (32*67)\n                #print(decoder_output.shape,curr_target_chars.shape)\n                #print(decoder_output.shape,curr_target_chars.shape)\n\n                curr_target_chars = target_batch[:, i] #(32)\n                curr_target_chars = curr_target_chars.type(dtype=torch.long)\n                #print(curr_target_chars)\n                \n                loss+=(lossFunction(decoder_output, curr_target_chars)) # you are passing 32*67 softmax values to curr_target_chars which has the 32*1\n                \n            tensor_2d = torch.stack(decoder_actual_output)\n            decoder_actual_output = tensor_2d.t() #it is outside the for loop\n            #print(decoder_actual_output) #32*40\n            if(batch_num == 0 and epoch == epochs-1):\n                numToCharConverter(target_batch,decoder_actual_output,data) \n                \n            train_accuracy += (decoder_actual_output == target_batch).all(dim=1).sum().item() # it is simple just summing up the equal values\n            train_loss += (loss.item()/output_seq_len)\n            \n            if(batch_num%200 == 0):\n                print(\"bt:\", batch_num, \" loss:\", loss.item()/output_seq_len)\n            #'k'/24\n            # here you get the actual word letters seqeunces softamx indeces\n            #[[0,1,2],[0,1,2]] = [shr,ram] 32*40\n            #correct = (decoder_actual_output == target_batch).all(dim=1).sum().item()\n            #accuracy = accuracy + correct\n            encoderOptimizer.zero_grad()\n            decoderOptimizer.zero_grad()\n            loss.backward()\n            encoderOptimizer.step()\n            decoderOptimizer.step()\n            \n        print(\"train_accuracy\",train_accuracy/512)\n        print(\"train_loss\",train_loss)\n        wandb.log({'train_accuracy':train_accuracy/512})\n        wandb.log({'train_loss':train_loss})\n        validationAccuracy(encoder,decoder,batchsize,tf_ratio)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T06:46:01.609486Z","iopub.execute_input":"2024-05-14T06:46:01.60987Z","iopub.status.idle":"2024-05-14T06:46:01.628789Z","shell.execute_reply.started":"2024-05-14T06:46:01.609829Z","shell.execute_reply":"2024-05-14T06:46:01.627526Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"\n\n\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n    \n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def numToCharConverter(inputArray,outputArray,data):\n    mp = data['num_char_map_2']\n    t1 = ''\n    t2 = ''\n    for row1, row2 in zip(inputArray,outputArray):\n        t1=''\n        t2=''\n        for e1, e2 in zip(row1,row2):\n            t1+=mp[e1.item()]\n            t2+=mp[e2.item()]\n        print(t1,\" \",t2)\n            \n        \n    ","metadata":{"execution":{"iopub.status.busy":"2024-05-14T06:42:40.570418Z","iopub.execute_input":"2024-05-14T06:42:40.571253Z","iopub.status.idle":"2024-05-14T06:42:40.577375Z","shell.execute_reply.started":"2024-05-14T06:42:40.571194Z","shell.execute_reply":"2024-05-14T06:42:40.576153Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def main_fun():\n    wandb.init(project ='vanillaRNN')\n    params = wandb.config\n    with wandb.init(project = 'vanillaRNN', name='embedding'+str(params.embSize)+'cellType'+params.cellType+'batchSize'+str(params.batchsize)) as run:\n        train(params.embSize,params.encoderLayers,params.decoderLayers,params.hiddenLayerNuerons,params.cellType,params.bidirection,params.dropout,params.epochs,params.batchsize,params.learningRate,params.optimizer,params.tf_ratio)\n    \nsweep_params = {\n    'method' : 'bayes',\n    'name'   : 'DeepLearningAssignment3',\n    'metric' : {\n        'goal' : 'maximize',\n        'name' : 'validation_accuracy',\n    },\n    'parameters' : {\n        'embSize':{'values':[16,32,64]},\n        'encoderLayers':{'values':[1,5,10]},\n        'decoderLayers' : {'values' : [1,5,10]},\n        'hiddenLayerNuerons'   : {'values' : [64,256,512]},\n        'cellType' : {'values' : ['GRU'] } ,\n        'bidirection' : {'values' : ['no']},\n        'dropout' : {'values' : [0.2,0.3]},\n        'epochs'  : {'values': [10,30,50]},\n        'batchsize' : {'values' : [32,64]},\n        'learningRate' : {'values' : [1e-2,1e-3,1e-4]},\n        'optimizer':{'values' : ['Adam','Nadam']},\n        'tf_ratio' :{'values' : [0.2,0.4,0.5]}\n    }\n}\nsweepId = wandb.sweep(sweep_params,project = 'vanillaRNN')\nwandb.agent(sweepId,function =main_fun,count = 2)\nwandb.finish()","metadata":{"execution":{"iopub.status.busy":"2024-05-14T06:47:11.55971Z","iopub.execute_input":"2024-05-14T06:47:11.560637Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n","output_type":"stream"},{"name":"stdout","text":"Create sweep with ID: p7xk0nii\nSweep URL: https://wandb.ai/dlassignment/vanillaRNN/sweeps/p7xk0nii\nVBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))\n<IPython.core.display.HTML object>\n<IPython.core.display.HTML object>\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5hweqpw6 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchsize: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbidirection: no\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcellType: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoderLayers: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembSize: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tencoderLayers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 30\n\u001b[34m\u001b[1mwandb\u001b[0m: \thiddenLayerNuerons: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearningRate: 0.001\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: Adam\n\u001b[34m\u001b[1mwandb\u001b[0m: \ttf_ratio: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\nException in thread ChkStopThr:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 286, in check_stop_status\n    self._loop_check_status(\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 224, in _loop_check_status\n    local_handle = request()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/interface/interface.py\", line 840, in deliver_stop_status\n    return self._deliver_stop_status(status)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/interface/interface_shared.py\", line 494, in _deliver_stop_status\n    return self._deliver_record(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/interface/interface_shared.py\", line 459, in _deliver_record\nException in thread IntMsgThr:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\nException in thread NetStatThr:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 953, in run\n        handle = mailbox._deliver_record(record, interface=self)    self.run()\nself._target(*self._args, **self._kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/lib/mailbox.py\", line 455, in _deliver_record\n\n  File \"/opt/conda/lib/python3.10/threading.py\", line 953, in run\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 300, in check_internal_messages\n        interface._publish(record)self._target(*self._args, **self._kwargs)    \n\nself._loop_check_status(  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/interface/interface_sock.py\", line 51, in _publish\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 268, in check_network_status\n\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 224, in _loop_check_status\n    self._loop_check_status(\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/wandb_run.py\", line 224, in _loop_check_status\n    self._sock_client.send_record_publish(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py\", line 221, in send_record_publish\n    local_handle = request()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/interface/interface.py\", line 856, in deliver_internal_messages\n    local_handle = request()\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/interface/interface.py\", line 848, in deliver_network_status\n    self.send_server_request(server_req)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py\", line 155, in send_server_request\n    return self._deliver_internal_messages(internal_message)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/interface/interface_shared.py\", line 516, in _deliver_internal_messages\n    self._send_message(msg)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py\", line 152, in _send_message\n    return self._deliver_record(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/interface/interface_shared.py\", line 459, in _deliver_record\n        return self._deliver_network_status(status)    handle = mailbox._deliver_record(record, interface=self)\nself._sendall_with_error_handle(header + data)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/interface/interface_shared.py\", line 510, in _deliver_network_status\n\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/lib/mailbox.py\", line 455, in _deliver_record\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py\", line 130, in _sendall_with_error_handle\n    interface._publish(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/interface/interface_sock.py\", line 51, in _publish\n    return self._deliver_record(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/interface/interface_shared.py\", line 459, in _deliver_record\n    sent = self._sock.send(data)\nBrokenPipeError: [Errno 32] Broken pipe\n    self._sock_client.send_record_publish(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py\", line 221, in send_record_publish\n    self.send_server_request(server_req)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py\", line 155, in send_server_request\n    handle = mailbox._deliver_record(record, interface=self)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/lib/mailbox.py\", line 455, in _deliver_record\n    interface._publish(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/interface/interface_sock.py\", line 51, in _publish\n    self._send_message(msg)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py\", line 152, in _send_message\n    self._sock_client.send_record_publish(record)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py\", line 221, in send_record_publish\n    self._sendall_with_error_handle(header + data)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py\", line 130, in _sendall_with_error_handle\n    self.send_server_request(server_req)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py\", line 155, in send_server_request\n    sent = self._sock.send(data)\nBrokenPipeError: [Errno 32] Broken pipe\n    self._send_message(msg)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py\", line 152, in _send_message\n    self._sendall_with_error_handle(header + data)\n  File \"/opt/conda/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py\", line 130, in _sendall_with_error_handle\n    sent = self._sock.send(data)\nBrokenPipeError: [Errno 32] Broken pipe\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240514_064718-5hweqpw6</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/dlassignment/vanillaRNN/runs/5hweqpw6' target=\"_blank\">classic-sweep-1</a></strong> to <a href='https://wandb.ai/dlassignment/vanillaRNN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/dlassignment/vanillaRNN/sweeps/p7xk0nii' target=\"_blank\">https://wandb.ai/dlassignment/vanillaRNN/sweeps/p7xk0nii</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/dlassignment/vanillaRNN' target=\"_blank\">https://wandb.ai/dlassignment/vanillaRNN</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/dlassignment/vanillaRNN/sweeps/p7xk0nii' target=\"_blank\">https://wandb.ai/dlassignment/vanillaRNN/sweeps/p7xk0nii</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/dlassignment/vanillaRNN/runs/5hweqpw6' target=\"_blank\">https://wandb.ai/dlassignment/vanillaRNN/runs/5hweqpw6</a>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Finishing last run (ID:5hweqpw6) before initializing another..."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb00b4b92ca64b188d022a365ac0c74d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">classic-sweep-1</strong> at: <a href='https://wandb.ai/dlassignment/vanillaRNN/runs/5hweqpw6' target=\"_blank\">https://wandb.ai/dlassignment/vanillaRNN/runs/5hweqpw6</a><br/> View project at: <a href='https://wandb.ai/dlassignment/vanillaRNN' target=\"_blank\">https://wandb.ai/dlassignment/vanillaRNN</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240514_064718-5hweqpw6/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Successfully finished last run (ID:5hweqpw6). Initializing new run:<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240514_064735-5hweqpw6</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/dlassignment/vanillaRNN/runs/5hweqpw6' target=\"_blank\">embedding64cellTypeGRUbatchSize64</a></strong> to <a href='https://wandb.ai/dlassignment/vanillaRNN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/dlassignment/vanillaRNN/sweeps/p7xk0nii' target=\"_blank\">https://wandb.ai/dlassignment/vanillaRNN/sweeps/p7xk0nii</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/dlassignment/vanillaRNN' target=\"_blank\">https://wandb.ai/dlassignment/vanillaRNN</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/dlassignment/vanillaRNN/sweeps/p7xk0nii' target=\"_blank\">https://wandb.ai/dlassignment/vanillaRNN/sweeps/p7xk0nii</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/dlassignment/vanillaRNN/runs/5hweqpw6' target=\"_blank\">https://wandb.ai/dlassignment/vanillaRNN/runs/5hweqpw6</a>"},"metadata":{}},{"name":"stdout","text":"bt: 0  loss: 4.226894544518513\nbt: 200  loss: 1.8207030918287195\nbt: 400  loss: 1.8083355115807576\nbt: 600  loss: 1.6731506015943445\ntrain_accuracy 1.880859375\ntrain_loss 1350.1659686047105\nbt: 0  loss: 1.4505419523819634\nbt: 20  loss: 1.5248487721318784\nbt: 40  loss: 1.4063297769297725\nbt: 60  loss: 0.3964451084966245\nvalidation_accuracy 6.54296875\nvalidation_loss 84.27477210500967\nbt: 0  loss: 1.7409896850585938\nbt: 200  loss: 1.4113021518873132\nbt: 400  loss: 1.557510873545771\nbt: 600  loss: 1.5796760890794836\ntrain_accuracy 14.2265625\ntrain_loss 1034.0812792985319\nbt: 0  loss: 1.1592445373535156\nbt: 20  loss: 0.12331232817276665\nbt: 40  loss: 1.1536732549252717\nbt: 60  loss: 1.1071439825970193\nvalidation_accuracy 12.4755859375\nvalidation_loss 64.11064842472905\nbt: 0  loss: 1.400758992070737\nbt: 200  loss: 1.397218123726223\nbt: 400  loss: 1.4387178835661516\nbt: 600  loss: 1.4976675614066746\ntrain_accuracy 19.173828125\ntrain_loss 927.5677470668496\nbt: 0  loss: 1.0188982590385105\nbt: 20  loss: 1.0779076451840608\nbt: 40  loss: 1.08246023758598\nbt: 60  loss: 1.027947467306386\nvalidation_accuracy 16.357421875\nvalidation_loss 58.582913657893336\nbt: 0  loss: 0.05263146628504214\nbt: 200  loss: 1.3652920930281929\nbt: 400  loss: 1.4163113469662874\nbt: 600  loss: 0.03159045913945074\ntrain_accuracy 19.6640625\ntrain_loss 894.2315949979043\nbt: 0  loss: 1.0819652391516643\nbt: 20  loss: 1.1179095558498218\nbt: 40  loss: 1.0388468866762908\nbt: 60  loss: 1.0525244837221892\nvalidation_accuracy 24.21875\nvalidation_loss 52.22747752977454\nbt: 0  loss: 0.024891057740087093\nbt: 200  loss: 1.3426393425982932\nbt: 400  loss: 1.3429901288903279\nbt: 600  loss: 1.2814833599588145\ntrain_accuracy 18.201171875\ntrain_loss 889.9547612861447\nbt: 0  loss: 1.065768863843835\nbt: 20  loss: 1.0463444253672725\nbt: 40  loss: 1.1089675737463909\nbt: 60  loss: 0.9852585170579993\nvalidation_accuracy 21.337890625\nvalidation_loss 52.635026470474585\nbt: 0  loss: 1.383438607920771\nbt: 200  loss: 1.4104460011357847\nbt: 400  loss: 0.01548948884010315\nbt: 600  loss: 1.3737943068794582\ntrain_accuracy 19.787109375\ntrain_loss 856.4652694489636\nbt: 0  loss: 1.07555281597635\nbt: 20  loss: 1.082003054411515\nbt: 40  loss: 1.0577364382536516\nbt: 60  loss: 1.069450046705163\nvalidation_accuracy 10.8642578125\nvalidation_loss 58.646613179341614\nbt: 0  loss: 1.3741785961648691\nbt: 200  loss: 1.278205622797427\nbt: 400  loss: 1.3151945860489556\nbt: 600  loss: 1.3086914394212805\ntrain_accuracy 18.72265625\ntrain_loss 846.2562341158799\nbt: 0  loss: 0.008724813228068144\nbt: 20  loss: 0.008460192576698635\nbt: 40  loss: 1.0759986379872197\nbt: 60  loss: 1.0364185830821162\nvalidation_accuracy 23.2421875\nvalidation_loss 49.46531021465425\nbt: 0  loss: 1.3552178921906843\nbt: 200  loss: 0.009288133486457493\nbt: 400  loss: 1.234530490377675\nbt: 600  loss: 0.00795875943225363\ntrain_accuracy 19.875\ntrain_loss 821.6207570375312\nbt: 0  loss: 0.9816104225490404\nbt: 20  loss: 0.8461162732995074\nbt: 40  loss: 0.007393976916437564\nbt: 60  loss: 0.9698329593824304\nvalidation_accuracy 24.8779296875\nvalidation_loss 47.71158551457133\nbt: 0  loss: 1.2145986142365828\nbt: 200  loss: 0.006681956674741662\nbt: 400  loss: 1.1552000460417375\nbt: 600  loss: 1.2668797866157864\ntrain_accuracy 18.029296875\ntrain_loss 829.3875903163256\nbt: 0  loss: 0.9449529233186141\nbt: 20  loss: 0.8610265980596128\nbt: 40  loss: 0.9905542290729025\nbt: 60  loss: 1.004097150719684\nvalidation_accuracy 13.9404296875\nvalidation_loss 53.34857443039833\nbt: 0  loss: 0.007263395449389582\nbt: 200  loss: 0.009298112729321356\nbt: 400  loss: 1.2457967841106912\nbt: 600  loss: 1.2792158541472063\ntrain_accuracy 19.9140625\ntrain_loss 799.7013802981895\nbt: 0  loss: 1.0171980650528618\nbt: 20  loss: 0.9154580157736073\nbt: 40  loss: 0.9605341372282609\nbt: 60  loss: 0.9937968046768851\nvalidation_accuracy 15.5517578125\nvalidation_loss 52.011218816689805\nbt: 0  loss: 1.1559894395911174\nbt: 200  loss: 1.2254218225893767\nbt: 400  loss: 1.2598604119342307\nbt: 600  loss: 1.2163073498269785\ntrain_accuracy 21.919921875\ntrain_loss 772.9112402200695\nbt: 0  loss: 0.952223653378694\nbt: 20  loss: 0.9045352106509001\nbt: 40  loss: 1.0012944262960684\nbt: 60  loss: 0.9390595477560292\nvalidation_accuracy 18.7255859375\nvalidation_loss 49.97823539343866\nbt: 0  loss: 1.315338549406632\nbt: 200  loss: 1.2740379831065303\nbt: 400  loss: 1.174839102703592\nbt: 600  loss: 1.1590363046397334\ntrain_accuracy 21.185546875\ntrain_loss 771.2062748103693\nbt: 0  loss: 0.928200680276622\nbt: 20  loss: 0.8961053101912789\nbt: 40  loss: 0.8571668707806132\nbt: 60  loss: 0.9340807873269786\nvalidation_accuracy 17.08984375\nvalidation_loss 50.22554171927596\nbt: 0  loss: 1.2156702124554177\nbt: 200  loss: 0.006953777178474094\nbt: 400  loss: 1.1883392333984375\nbt: 600  loss: 1.2470618538234546\ntrain_accuracy 17.833984375\ntrain_loss 801.4576007953805\nbt: 0  loss: 0.982185114984927\nbt: 20  loss: 0.9849994493567426\nbt: 40  loss: 0.971819255663001\nbt: 60  loss: 0.9036146246868632\nvalidation_accuracy 15.576171875\nvalidation_loss 50.72595477363337\nbt: 0  loss: 1.2498906177023184\nbt: 200  loss: 1.1871410867442256\nbt: 400  loss: 0.0050527236383894215\nbt: 600  loss: 1.2409731823465098\ntrain_accuracy 19.908203125\ntrain_loss 779.0196271506351\nbt: 0  loss: 0.8826899321182914\nbt: 20  loss: 0.8956167801566746\nbt: 40  loss: 0.004141773866570514\nbt: 60  loss: 0.0037809892193130827\nvalidation_accuracy 23.3154296875\nvalidation_loss 45.40361009570567\nbt: 0  loss: 0.0047516971826553345\nbt: 200  loss: 1.2844060814898948\nbt: 400  loss: 1.2349444679591968\nbt: 600  loss: 1.1245755734650984\ntrain_accuracy 18.484375\ntrain_loss 783.1897195946912\nbt: 0  loss: 0.889665935350501\nbt: 20  loss: 0.9271445067032523\nbt: 40  loss: 0.0036374660937682443\nbt: 60  loss: 0.9524573450503142\nvalidation_accuracy 23.291015625\nvalidation_loss 45.681045688688755\nbt: 0  loss: 1.2517288042151409\nbt: 200  loss: 1.207604283871858\nbt: 400  loss: 0.003974142281905464\nbt: 600  loss: 0.0038398961010186567\ntrain_accuracy 20.357421875\ntrain_loss 763.0276737086803\nbt: 0  loss: 0.9097682289455248\nbt: 20  loss: 0.936253837917162\nbt: 40  loss: 0.8966498167618461\nbt: 60  loss: 0.901345626167629\nvalidation_accuracy 17.1142578125\nvalidation_loss 49.224565299308814\nbt: 0  loss: 1.1728824117909307\nbt: 200  loss: 1.167617880779764\nbt: 400  loss: 1.207060026085895\nbt: 600  loss: 1.260782241821289\ntrain_accuracy 19.357421875\ntrain_loss 766.1860840511711\nbt: 0  loss: 1.069418782773225\nbt: 20  loss: 1.0067933953326682\nbt: 40  loss: 0.8958584329356318\nbt: 60  loss: 0.8374673594599185\nvalidation_accuracy 15.6494140625\nvalidation_loss 49.52724093295958\nbt: 0  loss: 1.178915853085725\nbt: 200  loss: 1.1009569582731829\nbt: 400  loss: 1.0781035215958306\nbt: 600  loss: 1.1975403661313264\ntrain_accuracy 22.712890625\ntrain_loss 731.2580184858775\nbt: 0  loss: 0.8995807481848676\nbt: 20  loss: 0.9289914006772249\nbt: 40  loss: 0.9700957588527513\nbt: 60  loss: 0.008090560203013212\nvalidation_accuracy 24.853515625\nvalidation_loss 43.79295735482289\nbt: 0  loss: 0.003295659047106038\nbt: 200  loss: 1.2027608622675356\nbt: 400  loss: 1.2511014523713484\nbt: 600  loss: 1.0786976192308508\ntrain_accuracy 19.73046875\ntrain_loss 755.1253435955101\nbt: 0  loss: 0.9648903556492018\nbt: 20  loss: 0.0030988626506017604\nbt: 40  loss: 0.8689018747080928\nbt: 60  loss: 1.0099124079165251\nvalidation_accuracy 24.90234375\nvalidation_loss 43.60330492565813\nbt: 0  loss: 0.0038914524990579357\nbt: 200  loss: 1.125523857448412\nbt: 400  loss: 1.1692310830821162\nbt: 600  loss: 1.1521840302840523\ntrain_accuracy 20.974609375\ntrain_loss 741.1620316975141\nbt: 0  loss: 0.9395340629245924\nbt: 20  loss: 0.9518149832020635\nbt: 40  loss: 0.7989725859268851\nbt: 60  loss: 1.0012121615202532\nvalidation_accuracy 18.6279296875\nvalidation_loss 48.70511103000329\nbt: 0  loss: 1.2237799271293308\nbt: 200  loss: 1.2006518322488535\nbt: 400  loss: 1.15850713978643\nbt: 600  loss: 1.1625542848006538\ntrain_accuracy 20.9609375\ntrain_loss 741.7977656252348\nbt: 0  loss: 0.8752341063126273\nbt: 20  loss: 0.003084113740402719\nbt: 40  loss: 0.9298967278521993\nbt: 60  loss: 0.007557836563690849\nvalidation_accuracy 26.46484375\nvalidation_loss 42.14763422990623\nbt: 0  loss: 1.1199010766070823\nbt: 200  loss: 1.18213180873705\nbt: 400  loss: 0.0029142100525938945\nbt: 600  loss: 1.047102637912916\ntrain_accuracy 19.361328125\ntrain_loss 747.2831347845496\nbt: 0  loss: 0.0027690179969953456\nbt: 20  loss: 0.9110786603844684\nbt: 40  loss: 0.9637998498004415\nbt: 60  loss: 0.9523416602093241\nvalidation_accuracy 18.7255859375\nvalidation_loss 46.318261077708506\nbt: 0  loss: 1.0825071749479875\nbt: 200  loss: 1.0880939649498982\nbt: 400  loss: 1.198541309522546\nbt: 600  loss: 1.1148333342179009\ntrain_accuracy 20.10546875\ntrain_loss 736.8851759466139\nbt: 0  loss: 0.00266344683325809\nbt: 20  loss: 0.900311511495839\nbt: 40  loss: 0.8399353856625764\nbt: 60  loss: 0.9044329601785411\nvalidation_accuracy 20.3125\nvalidation_loss 45.47016567591091\nbt: 0  loss: 1.2380167919656504\nbt: 200  loss: 1.2199894448985225\nbt: 400  loss: 1.2177152219025984\nbt: 600  loss: 0.002862889157689136\ntrain_accuracy 19.3671875\ntrain_loss 742.6513760425146\nbt: 0  loss: 0.8598820230235225\nbt: 20  loss: 0.8903097899063773\nbt: 40  loss: 1.0159800156303074\nbt: 60  loss: 0.002584791053896365\nvalidation_accuracy 17.2119140625\nvalidation_loss 46.90897650466018\nbt: 0  loss: 1.1340964773426885\nbt: 200  loss: 0.003576921056146207\nbt: 400  loss: 1.1270619268002717\nbt: 600  loss: 1.1287554865298064\ntrain_accuracy 21.841796875\ntrain_loss 712.2253685327972\nbt: 0  loss: 0.8261626699696416\nbt: 20  loss: 0.8285549827243971\nbt: 40  loss: 0.8470910943072775\nbt: 60  loss: 0.8845750559931216\nvalidation_accuracy 14.0625\nvalidation_loss 47.757530236697725\nbt: 0  loss: 1.1139259338378906\nbt: 200  loss: 0.0027829628923664923\nbt: 400  loss: 1.0649749092433765\nbt: 600  loss: 1.1017878159232761\ntrain_accuracy 18.99609375\ntrain_loss 728.4923935545534\nbt: 0  loss: 0.8069237418796705\nbt: 20  loss: 0.8335727194081182\nbt: 40  loss: 0.002548547704582629\nbt: 60  loss: 0.8823300237240999\nvalidation_accuracy 20.3125\nvalidation_loss 43.92663587791765\nbt: 0  loss: 1.0755715577498726\nbt: 200  loss: 1.110622737718665\nbt: 400  loss: 0.0025526722488196\nbt: 600  loss: 1.0705093715501868\ntrain_accuracy 21.869140625\ntrain_loss 698.5408077031042\nbt: 0  loss: 0.8994846343994141\nbt: 20  loss: 0.820831879325535\nbt: 40  loss: 0.8116393711255945\nbt: 60  loss: 0.8597077908723251\nvalidation_accuracy 20.361328125\nvalidation_loss 43.79730727021461\nbt: 0  loss: 1.10656680231509\nbt: 200  loss: 1.1014271611752717\nbt: 400  loss: 1.0560469420059868\nbt: 600  loss: 1.0887500099513843\ntrain_accuracy 19.37890625\ntrain_loss 711.8015075990032\nbt: 0  loss: 0.0024197730033294015\nbt: 20  loss: 0.7608922875445822\nbt: 40  loss: 0.8686905736508577\nbt: 60  loss: 0.9048771236253821\nvalidation_accuracy 14.0869140625\nvalidation_loss 46.765434492379434\nbt: 0  loss: 0.9873460686725118\nbt: 200  loss: 1.05547979603643\nbt: 400  loss: 1.0183708356774372\nbt: 600  loss: 0.0030083387442257094\ntrain_accuracy 19.14453125\ntrain_loss 711.1906269416206\nbt: 0  loss: 0.8951276696246603\nbt: 20  loss: 0.0021621000183665233\nbt: 40  loss: 0.7393496140189793\nbt: 60  loss: 0.8166131558625594\nvalidation_accuracy 15.7958984375\nvalidation_loss 44.8433971741925\n{గ్లిచ్చెస్ను}}}}}}}}}}   {గుర్కిస్తాన}}}}}}}}}}}\n{బిరిమడ్గియ}}}}}}}}}}}}   {బిర్రా్ల్న}}}}}}}}}}}}\n{కొనియాడుతున్నామని}}}}}   {కొల్లింలున్నననాన}}}}}}\n{ఆరాధకులైనప్పటికీ}}}}}}   {ఆరానిిల్ననననాాన}}}}}}}\n{జంగ్వీ}}}}}}}}}}}}}}}}   {జంగుని}}}}}}}}}}}}}}}}\n{గాయపడ్డారని}}}}}}}}}}}   {గాిరా్ాలన}}}}}}}}}}}}}\n{నికాన్}}}}}}}}}}}}}}}}   {నిర్న్}}}}}}}}}}}}}}}}\n{వచ్చేసిందనుకున్న}}}}}}   {వర్కింింుుునననాన}}}}}}\n{శ్రీప్రదం}}}}}}}}}}}}}   {స్రీవియాన}}}}}}}}}}}}}\n{రచిస్తుందట}}}}}}}}}}}}   {రాజుచింంంది}}}}}}}}}}}\n{ఆర్ట్స్పై}}}}}}}}}}}}}   {అర్క్స్న్}}}}}}}}}}}}}\n{వేగానర్}}}}}}}}}}}}}}}   {వారన్్}}}}}}}}}}}}}}}}\n{మూలమేంటి}}}}}}}}}}}}}}   {మొందాలని}}}}}}}}}}}}}}\n{విశ్రాంతికొచ్చిందనీ}}}   {విశ్వయించచచచననాాని}}}}\n{వెళ్లుల్లి}}}}}}}}}}}}   {వెల్లులున్}}}}}}}}}}}}\n{స్థిమితపడింది}}}}}}}}}   {స్వికించిిిని}}}}}}}}}\n{చందాదారుడు}}}}}}}}}}}}   {చందిలినాని}}}}}}}}}}}}\n{పొంగిస్తున్నా}}}}}}}}}   {పొంగులుుునాన}}}}}}}}}}\n{మనుషులచ్చి}}}}}}}}}}}}   {మదిచింంంి}}}}}}}}}}}}}\n{ఉద్యోగించి}}}}}}}}}}}}   {ఉద్తించంని}}}}}}}}}}}}\n{ఎక్కడన్నదే}}}}}}}}}}}}   {ఎక్కకునుాు}}}}}}}}}}}}\n{ఏఎస్ఎంఆర్}}}}}}}}}}}}}   {అస్ష్స్}}}}}}}}}}}}}}}\n{ఎవరుంటారనే}}}}}}}}}}}}   {ఎవర్పునాను}}}}}}}}}}}}\n{కిక్కుక్కు}}}}}}}}}}}}   {కుక్కు}}}}}}}}}}}}}}}}\n{బలహీనపడుతాయని}}}}}}}}}   {బాిిిలననననాన}}}}}}}}}}\n{కనపడకూడని}}}}}}}}}}}}}   {కలగిలుంని}}}}}}}}}}}}}\n{తప్పుతాడని}}}}}}}}}}}}   {తగ్పుంుంని}}}}}}}}}}}}\n{పరెపల్లి}}}}}}}}}}}}}}   {పరిలులలు}}}}}}}}}}}}}}\n{గొరిల్లాల్లో}}}}}}}}}}   {గెల్ల్ల్లలల}}}}}}}}}}}\n{వ్యూహాలనే}}}}}}}}}}}}}   {విరారా}}}}}}}}}}}}}}}}\n{యాక్సెలరేటర్}}}}}}}}}}   {ఎత్ర్రా్్న}}}}}}}}}}}}\n{వెలిగారో}}}}}}}}}}}}}}   {వెళ్లాు}}}}}}}}}}}}}}}\n{బతకాలిరా}}}}}}}}}}}}}}   {బా్ాాలి}}}}}}}}}}}}}}}\n{చరితనంతా}}}}}}}}}}}}}}   {చర్వింని}}}}}}}}}}}}}}\n{పాలిసీలు}}}}}}}}}}}}}}   {పెల్లిని}}}}}}}}}}}}}}\n{స్పోక్ల}}}}}}}}}}}}}}}   {స్కుకున}}}}}}}}}}}}}}}\n{గురుద్రోహిని}}}}}}}}}}   {గుర్కిస్న్ని}}}}}}}}}}\n{ఆరితేరినాడు}}}}}}}}}}}   {ఆరిమియానాన}}}}}}}}}}}}\n{అర్థార్థులు}}}}}}}}}}}   {అర్కా్్్లలు}}}}}}}}}}}\n{చూసుకునేవాడిని}}}}}}}}   {చూసుకుకుం్ందన}}}}}}}}}\n{శోభించాయి}}}}}}}}}}}}}   {సిపించిన}}}}}}}}}}}}}}\n{వేడెక్కుతున్నాయ}}}}}}}   {వెలుకుకున్నానాన}}}}}}}\n{స్థలాల్లేవు}}}}}}}}}}}   {స్వా్ల్ల్లు}}}}}}}}}}}\n{యెరోషికు}}}}}}}}}}}}}}   {యోరిసిసి}}}}}}}}}}}}}}\n{ఇవ్వలేము}}}}}}}}}}}}}}   {అమ్వాాను}}}}}}}}}}}}}}\n{ప్రతిపేజీలో}}}}}}}}}}}   {ప్రాా్ంంంన}}}}}}}}}}}}\n{చివరనుంటే}}}}}}}}}}}}}   {చినా్లాలన}}}}}}}}}}}}}\n{పరిత్యాగంలో}}}}}}}}}}}   {పరిర్్్ల్న}}}}}}}}}}}}\n{స్తంభాలలు}}}}}}}}}}}}}   {స్వాన్లల}}}}}}}}}}}}}}\n{ఎక్స్ట్రానోడల్}}}}}}}}   {ఎక్క్ర్ర్్్ను}}}}}}}}}\n{చివరాఖరుకు}}}}}}}}}}}}   {చిరా్్్్న}}}}}}}}}}}}}\n{తిప్పికొడుతోంది}}}}}}}   {తొలుచుకున్ంంంన}}}}}}}}\n{లొకాలిటీని}}}}}}}}}}}}   {లిన్రియాని}}}}}}}}}}}}\n{తిరుమంగలంలోని}}}}}}}}}   {తిర్లలునుుాన}}}}}}}}}}\n{నిరూపిస్తుంటే}}}}}}}}}   {నిర్చిపునానాి}}}}}}}}}\n{త్రగ్గుతున్నట్లు}}}}}}   {త్రాగుకుుుుున్ాన}}}}}}\n{దాడిగానే}}}}}}}}}}}}}}   {దారింంన}}}}}}}}}}}}}}}\n{నంబర్లుకు}}}}}}}}}}}}}   {నంగుల్లలు}}}}}}}}}}}}}\n{రంజింపచేయాలనే}}}}}}}}}   {రందిిిిినాాన}}}}}}}}}}\n{కృతజ్ఞతలని}}}}}}}}}}}}   {కురక్్్లలు}}}}}}}}}}}}\n{డ్రాస్టికల్}}}}}}}}}}}   {ద్వార్్్న}}}}}}}}}}}}}\n{వాదనుంది}}}}}}}}}}}}}}   {వాలులును}}}}}}}}}}}}}}\n{బాణగంగా}}}}}}}}}}}}}}}   {బాలలలన}}}}}}}}}}}}}}}}\n{ట్రాఫిక్లు}}}}}}}}}}}}   {త్రార్్్న్}}}}}}}}}}}}\nbt: 0  loss: 1.0682335729184358\nbt: 200  loss: 0.003145401244578154\nbt: 400  loss: 1.1903889697530996\nbt: 600  loss: 1.0693666209345278\ntrain_accuracy 21.380859375\ntrain_loss 686.7520587322828\nbt: 0  loss: 0.873528936634893\nbt: 20  loss: 0.848637871120287\nbt: 40  loss: 0.002239626224922097\nbt: 60  loss: 0.0022956722456475964\nvalidation_accuracy 21.9482421875\nvalidation_loss 41.480123175389096\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efaaa300815c403b9c7d1f259a73a0e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_accuracy</td><td>▁▅▇▇▆▇▇▇▆▇█▇▆▇▇▇▇█▇▇▇▇▇▇█▇█▇▇█</td></tr><tr><td>train_loss</td><td>█▅▄▃▃▃▃▂▃▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▃▄▇▆▃▇▇▄▄▅▅▄▇▇▅▄▇▇▅█▅▆▅▄▆▆▄▄▆</td></tr><tr><td>validation_loss</td><td>█▅▄▃▃▄▂▂▃▃▂▂▃▂▂▂▂▁▁▂▁▂▂▂▂▁▁▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_accuracy</td><td>21.38086</td></tr><tr><td>train_loss</td><td>686.75206</td></tr><tr><td>validation_accuracy</td><td>21.94824</td></tr><tr><td>validation_loss</td><td>41.48012</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">embedding64cellTypeGRUbatchSize64</strong> at: <a href='https://wandb.ai/dlassignment/vanillaRNN/runs/5hweqpw6' target=\"_blank\">https://wandb.ai/dlassignment/vanillaRNN/runs/5hweqpw6</a><br/> View project at: <a href='https://wandb.ai/dlassignment/vanillaRNN' target=\"_blank\">https://wandb.ai/dlassignment/vanillaRNN</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240514_064735-5hweqpw6/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 7zysbfrf with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchsize: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbidirection: no\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcellType: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoderLayers: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembSize: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tencoderLayers: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 50\n\u001b[34m\u001b[1mwandb\u001b[0m: \thiddenLayerNuerons: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearningRate: 0.001\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: Adam\n\u001b[34m\u001b[1mwandb\u001b[0m: \ttf_ratio: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240514_065908-7zysbfrf</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/dlassignment/vanillaRNN/runs/7zysbfrf' target=\"_blank\">eager-sweep-2</a></strong> to <a href='https://wandb.ai/dlassignment/vanillaRNN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/dlassignment/vanillaRNN/sweeps/p7xk0nii' target=\"_blank\">https://wandb.ai/dlassignment/vanillaRNN/sweeps/p7xk0nii</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/dlassignment/vanillaRNN' target=\"_blank\">https://wandb.ai/dlassignment/vanillaRNN</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/dlassignment/vanillaRNN/sweeps/p7xk0nii' target=\"_blank\">https://wandb.ai/dlassignment/vanillaRNN/sweeps/p7xk0nii</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/dlassignment/vanillaRNN/runs/7zysbfrf' target=\"_blank\">https://wandb.ai/dlassignment/vanillaRNN/runs/7zysbfrf</a>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Finishing last run (ID:7zysbfrf) before initializing another..."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2dd026a78bee4434b9013e1a693b5a39"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">eager-sweep-2</strong> at: <a href='https://wandb.ai/dlassignment/vanillaRNN/runs/7zysbfrf' target=\"_blank\">https://wandb.ai/dlassignment/vanillaRNN/runs/7zysbfrf</a><br/> View project at: <a href='https://wandb.ai/dlassignment/vanillaRNN' target=\"_blank\">https://wandb.ai/dlassignment/vanillaRNN</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240514_065908-7zysbfrf/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Successfully finished last run (ID:7zysbfrf). Initializing new run:<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240514_065925-7zysbfrf</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/dlassignment/vanillaRNN/runs/7zysbfrf' target=\"_blank\">embedding64cellTypeGRUbatchSize64</a></strong> to <a href='https://wandb.ai/dlassignment/vanillaRNN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/dlassignment/vanillaRNN/sweeps/p7xk0nii' target=\"_blank\">https://wandb.ai/dlassignment/vanillaRNN/sweeps/p7xk0nii</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/dlassignment/vanillaRNN' target=\"_blank\">https://wandb.ai/dlassignment/vanillaRNN</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/dlassignment/vanillaRNN/sweeps/p7xk0nii' target=\"_blank\">https://wandb.ai/dlassignment/vanillaRNN/sweeps/p7xk0nii</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/dlassignment/vanillaRNN/runs/7zysbfrf' target=\"_blank\">https://wandb.ai/dlassignment/vanillaRNN/runs/7zysbfrf</a>"},"metadata":{}},{"name":"stdout","text":"bt: 0  loss: 4.138681494671365\nbt: 200  loss: 1.5060567441193953\nbt: 400  loss: 1.589910424273947\nbt: 600  loss: 1.4415138908054517\ntrain_accuracy 0.0\ntrain_loss 1237.5827971748677\nbt: 0  loss: 1.1126754594885784\nbt: 20  loss: 1.2093979379405146\nbt: 40  loss: 1.156596556953762\nbt: 60  loss: 1.2054286625074304\nvalidation_accuracy 0.0\nvalidation_loss 72.85178574271825\nbt: 0  loss: 1.339441962864088\nbt: 200  loss: 1.3588578597359036\nbt: 400  loss: 1.3025303716244905\nbt: 600  loss: 1.232225749803626\ntrain_accuracy 0.0\ntrain_loss 1069.9040426171346\nbt: 0  loss: 1.0503410671068274\nbt: 20  loss: 0.8863300655199133\nbt: 40  loss: 0.990118524302607\nbt: 60  loss: 0.915020403654679\nvalidation_accuracy 0.0\nvalidation_loss 60.11573866139287\nbt: 0  loss: 1.257032062696374\nbt: 200  loss: 1.2118649690047554\nbt: 400  loss: 1.0629578466000764\nbt: 600  loss: 0.97290097112241\ntrain_accuracy 0.119140625\ntrain_loss 860.2513406173042\nbt: 0  loss: 0.6160142732703168\nbt: 20  loss: 0.6024523610654084\nbt: 40  loss: 0.5612928556359332\nbt: 60  loss: 0.6839452826458475\nvalidation_accuracy 1.66015625\nvalidation_loss 40.648062415744945\nbt: 0  loss: 0.8242879950481913\nbt: 200  loss: 0.7311861618705418\nbt: 400  loss: 0.7065943013066831\nbt: 600  loss: 0.6876844323199728\ntrain_accuracy 4.494140625\ntrain_loss 522.6735398251077\nbt: 0  loss: 0.3884017363838527\nbt: 20  loss: 0.39662585051163385\nbt: 40  loss: 0.4505866092184316\nbt: 60  loss: 0.46893546892249066\nvalidation_accuracy 14.453125\nvalidation_loss 25.775024310402248\nbt: 0  loss: 0.45797430950662366\nbt: 200  loss: 0.41283661386241083\nbt: 400  loss: 0.2669841724893321\nbt: 600  loss: 0.43198515021282696\ntrain_accuracy 18.765625\ntrain_loss 326.81574049721576\nbt: 0  loss: 0.38391436701235565\nbt: 20  loss: 0.35425227621327277\nbt: 40  loss: 0.45254280256188434\nbt: 60  loss: 0.3033150382663893\nvalidation_accuracy 21.7041015625\nvalidation_loss 19.827657720317017\nbt: 0  loss: 0.3790561841881793\nbt: 200  loss: 0.3766554542209791\nbt: 400  loss: 0.31464634770932404\nbt: 600  loss: 0.40280644789985987\ntrain_accuracy 33.041015625\ntrain_loss 245.1149613546289\nbt: 0  loss: 0.3253499528636103\nbt: 20  loss: 0.31040311896282696\nbt: 40  loss: 0.27740940840347955\nbt: 60  loss: 0.3478024731511655\nvalidation_accuracy 43.798828125\nvalidation_loss 15.57599902800892\nbt: 0  loss: 0.29308453850124194\nbt: 200  loss: 0.24747757289720618\n","output_type":"stream"}]},{"cell_type":"code","source":"\ntrain(data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}